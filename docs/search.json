[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Spatial analysis of public health data",
    "section": "Welcome",
    "text": "Welcome\nHello, this website  holds the material used to support the Spatial Analysis of public health data, part of the MSc in Modelling for Global Health at the University of Oxford."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "The Bartlett Centre for Advanced Spatial Analysis, https://twitter.com/andymaclachlan↩︎"
  },
  {
    "objectID": "software.html#qgis",
    "href": "software.html#qgis",
    "title": "Software installation",
    "section": "QGIS",
    "text": "QGIS\nQGIS is an open-source graphic user interface GIS with many community developed add on packages that (or plugins) that provide additional functionality to the software. In this module we will primarly use QGIS to check our spatial data.\nTo get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html\nI install the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates.\nWhen you click through the dialogue boxes you need to search for QGIS in the OSGeo4W setup and click the refresh button so it changes from skip to install….\n\n\n\n\n\nContinuous and discrete data. Source: GIS Stackexchange"
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Software installation",
    "section": "R",
    "text": "R\nR is both a programming language and software environment, originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks.\n\nBasics\nThe advised method for using R is to download it to your personal machine, so you can use it in future without any issues.\nTo use “R” we need to bits of software:\n\nR itself: https://cran.rstudio.com/\nThen also RStudio: https://www.rstudio.com/products/rstudio/download/#download\n\nR is the core software and RStudio gives a simpler interface for us to use it. If you used R (which you could) you’d be just typing code, with no other features. RStudio gives us other features to make this much easier, just like how you would never type a report in notepad (Windows) or notes (on Apple), you’d use the word processor to make it easier. Chester Ismay and Albert Y. Kim explain this with a car analogy:\n\n\n\n\n\nR vs RStudio. Source: A ModernDive into R and the Tidyverse, Ismay and Kim, 2021\n\n\n\n\nR and RStudio are software that require updating over time. It’s easy to forget!\n\n\nPackages\nBase R (controlled or driven through RStudio) is very limited. As a result people develop packages to make data loading, wrangling, analysis and visulisation much easier than having to write all the code. All packages will have lots of functions that just look up code from the package to make what you are doing easier (we will go into this in more detail later).\nPackages develop over time and have different versions (like software). This can cause some issues with updates and code not working like it did once before. This book means i have to run all the code anyway, so every year it is checked and most packages are updated.\nThere are many errors and issues that can arise with package installation. You should read the following section now so you are aware of it, but you won’t need it until you hit an issue.\nThe most common package errors and issues are:\n\nThe package relies on another package which you might not have — this is called a dependency. Think about building a house — moving in and living in the house is dependent on the foundations, the walls, the windows. The solution is to install the package you are missing.\nThe package has a new version — simply update it, in RStudio the package tab (which we will see in the module) has an update option.\nYour version of R might not support the package. This is unlikely as I update this every year. But to solve it either use an older version of the package and/or update your version of R.\nThere is an error with the package. If all else fails i remove the package and the re-install it. Again, in the package tab there is a small cross next to each package, clicking it let’s you remove it.\n\nIf the problem persists then assuming the error is package xxx is not available consult the relevant stack overflow question"
  },
  {
    "objectID": "01_geographic_data.html#the-basics-of-geographic-information",
    "href": "01_geographic_data.html#the-basics-of-geographic-information",
    "title": "1  Geographic Information",
    "section": "1.1 The Basics of Geographic Information",
    "text": "1.1 The Basics of Geographic Information\nGeographic data, geospatial data or geographic information is data that identifies the location of features on Earth. There are two main types of data which are used in GIS applications to represent the real world. Vectors that are composed of points, lines and polygons and rasters that are grids of cells with individual values…\n\n\n\n\n\nTypes of spatial data. Source: Spatial data models\n\n\n\n\nIn the above example the features in the real world (e.g. lake, forest, marsh and grassland) have been represented by points, lines and polygons (vector) or discrete grid cells (raster) of a certain size (e.g. 1 x 1m) specifying land cover type.\n\n1.1.1 Data types in statistics\nBefore we go any further let’s just quick go over the different types of data you might encounter\nContinuous data can be measured on some sort of scale and can have any value on it such as height and weight.\nDiscrete data have finite values (meaning you can finish counting something). It is numeric and countable such as number of shoes or the number of computers within the classroom.\nFoot length would be continuous data but shoe size would be discrete data.\n\n\n\n\n\nContinuous and discrete data. Source: Allison Horst data science and stats illustrations\n\n\n\n\nNominal (also called categorical) data has labels without any quantitative value such as hair colour or type of animal. Think names or categories - there are no numbers here.\nOrdinal, similar to categorical but the data has an order or scale, for example if you have ever seen the chilli rating system on food labels or filled a happiness survey with a range between 1 and 10 — that’s ordinal. Here the order matters, but not the difference between them.\nBinary data is that that can have only two possible outcomes, yes and no or shark and not shark.\n\n\n\n\n\nNominal, ordinal and binary data. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\n\n1.1.2 Important GIS data formats\nThere are a number of commonly used geographic data formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them.\n\n1.1.2.1 Shapefiles\nPerhaps the most commonly used GIS data format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications.\nA shapefile is actually a collection of files —- at least three of which are needed for the shapefile to be displayed by GIS software. They are:\n\n.shp - the file which contains the feature geometry\n.shx - an index file which stores the position of the feature IDs in the .shp file\n.dbf - the file that stores all of the attribute information associated with the coordinates – this might be the name of the shape or some other information associated with the feature\n.prj - the file which contains all of the coordinate system information (the location of the shape on Earth’s surface). Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used\n\nOn Twitter and want to see the love for shapefiles….have a look at the shapefile account\n\n\n1.1.2.2 GeoJSON\nGeoJSON Geospatial Data Interchange format for JavaScript Object Notation is becoming an increasingly popular spatial data format, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file.\n\n\n1.1.2.3 Raster data\nMost raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostarionary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type.\n\n\n1.1.2.4 Geodatabase\nA geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab.\n\n\n\n\n\n\n\n\n\n\n\n1.1.2.5 GeoPackage\n\n\n\n\n\nGeoPacakge logo\n\n\n\n\nA GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move.\n\n\n1.1.2.6 SpatiaLite\n\n\n\n\n\nSpatialLite logo\n\n\n\n\nSpatialLite is an open-source library that extends SQLite core. Support is fairly limited and most software that supports SpatiaLite also supports GeoPackage, as they both build upon SQLite. It doesn’t have any clear advantage over GeoPackage, however it is unable to support raster data.\n\n\n1.1.2.7 PostGIS\n\n\n\n\n\nPostGIS logo\n\n\n\n\nPostGIS is an opensource database extender for PostrgeSQL. Essentially PostgreSQL is a database and PostGIS is an add on which permits spatial functions. The advantages of using PostGIS over a GeoPackage are that it allows users to access the data at the same time, can handle large data more efficiently and reduces processing time. In this example calculating the number of bars per neighbourhood in Leon, Mexico the processing time reduced from 1.443 seconds (SQLite) to 0.08 seconds in PostGIS. However, data stored in PostGIS is much harder to share, move or copy.\n\n\n1.1.2.8 What will I use\nThe variety of data formats can see a bit overwhelming. I still have to check how to load some of these data formats that I don’t use frequently. But don’t worry, most of the time you’ll be using shapefiles, GeoPackages or raster data."
  },
  {
    "objectID": "01_geographic_data.html#general-data-flow",
    "href": "01_geographic_data.html#general-data-flow",
    "title": "1  Geographic Information",
    "section": "1.2 General data flow",
    "text": "1.2 General data flow\nAs Grolemund and Wickham state in R for Data Science…\n\n“Data science is a huge field, and there’s no way you can master it by reading a single book.”\n\nHowever, a nice place to start is looking at the typical workflow of a data science (or GIS) project which you will see throughout these practicals, which is summarised nicely in this diagram produced by Dr. Julia Lowndes adapted from Grolemund and Wickham.\n\n\n\n\n\nUpdated from Grolemund & Wickham’s classis R4DS schematic, envisioned by Dr. Julia Lowndes for her 2019 useR! keynote talk and illustrated by Allison Horst. Source: Allison Horst data science and stats illustrations\n\n\n\n\nTo begin you have to import your data (not necessarily environmental) into R or some other kind of GIS to actually be able to do any kind of analysis on it.\nOnce imported you might need to tidy the data. This really depends on what kind of data it is and we cover this later on in the course. However, putting all of your data into a consistent structure will be very beneficial when you have to do analysis on it — as you can treat it all in the same way. Grolemund and Wickham state that data is tidy when “each column is a variable, and each row is an observation”, we cover this more in next week in the [Tidying data] section.\nWhen you have (or haven’t) tidied data you then will most likely want to transform it. Grolemund and Wickham define this as “narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means)”. However, from a GIS point of view I would also include putting all of your data into a similar projection, covered next week in [Changing projections] and any other basic process you might do before the core analysis. Arguably these processes could include things such as: clipping (cookie cutting your study area), buffering (making areas within a distance of a point) and intersecting (where two datasets overlap).\nTidying and transform = data wrangling. Remember from the introduction this could be 50-80% of a data science job!\n\n\n\n\n\ndplyr introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nAfter you have transformed the data the next best thing to do is visualise it — even with some basic summary statistics. This simple step will often let you look at your data in a different way and select more appropriate analysis.\nNext up is modelling. Personally within GIS i’d say a better term is processing as the very data itself is usually a computer model of reality. The modelling or processing section is where you conduct the core analysis (more than the basic analysis already mentioned) and try to provide an answer to your research question.\nFinally you have to communicate your study and outputs, it doesn’t matter how good your data wrangling, modelling or processing is, if your intended audience can’t interpret it, well, it’s pretty much useless."
  },
  {
    "objectID": "01_geographic_data.html#uk-spatial-geography",
    "href": "01_geographic_data.html#uk-spatial-geography",
    "title": "1  Geographic Information",
    "section": "1.3 UK spatial geography",
    "text": "1.3 UK spatial geography\nIn this practical we are going to take some regular data (without any geometry) and join it to a spatial data set so we can map it!\nFirstly we need spatial data. It can be quite a daunting task to attempt to understand all of the boundaries that are in use in England and Wales….briefly:\nStatistical hierarchy\n\nStatistical hierarchy are units that census data is collected, the smallest being an output area with around 100 residents.\nOutput areas can be aggregated to Lower Super Output Areas (LSOAs) with between 1,000 and 3,000 residents. These can be further aggregated to Middle Super Output Areas (MSOAs).\nOutput areas and LSOAs typically fit within administrative electoral wards (below)…\nWards and MSOAs fit within local authority areas\n\n\n\n\nNesting areas\n\n\n\n\n\nNesting example\n\n\n\nNote that all the boundaries can change (e.g. Some LSOAs between the 2011 census and 2021 census moved). To account for this we can use lookup tables to match the new areas with the old ones.\n\nAdministarive hierarchy\n\nAdministrative areas are based on government areas and this depends on where you are in England….\n\n\n\n\nA Beginner’s Guide to UK Geography\n\n\n\nSome parts of England have a two tier structure. Counties take on expensive services - such as education and transport. Whilst local authority districts took on smaller services - such as planning permission, markets and local roads. Under all of this are electoral wards that have local Councillors…\nIn 1974 a two tier system of counties and districts was enacted across England and Wales. In urban areas these were metropolitan counties and metropolitan districts..\nBut in 1986 the metropolitan counties were removed (although still recognised) and the metropolitan districts were left as a single authority.\nFrom 1990 many of the tier structures (not in metropolitan areas) were combined into a single structure called Unitary Authorities, especially in medium-sized urban areas. However, some still retained the two tier structure.\n\nAn easy to read guide on census / administrative geography was produced by the London Borough of Tower Hamlets - skip to page 2 for a visual summary."
  },
  {
    "objectID": "01_geographic_data.html#r-spatial-data-intro",
    "href": "01_geographic_data.html#r-spatial-data-intro",
    "title": "1  Geographic Information",
    "section": "1.4 R Spatial data intro",
    "text": "1.4 R Spatial data intro\nR has a very well developed ecosystem of packages for working with Spatial Data. Early pioneers like Roger Bivand and Edzer Pebesma along with various colleagues were instrumental in writing packages to interface with some powerful open source libraries for working with spatial data, such as GDAL and GEOS. These were accessed via the rgdal and rgeos packages. The maptools package by Roger Bivand, amongst other things, allowed Shapefiles to be read into R. The sp package (along with spdep) by Edzer Pebesma was very important for defining a series of classes and methods for spatial data natively in R which then allowed others to write software to work with these formats. Many these original packages were retired (and superseded by the ones we will use today) at the end of 2023 as their maintainer Roger Bivand also retired. Other packages like raster advanced the analysis of gridded spatial data, while packages like classInt and RColorbrewer facilitated the binning of data and colouring of choropleth maps.\nWhilst these packages were extremely important for advancing spatial data analysis in R, they were not always the most straightforward to use — making a map in R could take quite a lot of effort and they were static and visually basic. However, more recently new packages have arrived to change this. Now leaflet enables R to interface with the leaflet javascript library for online, dynamic maps. ggplot2 which was developed by Hadley Wickham and colleagues radically changed the way that people thought about and created graphical objects in R, including maps, and introduced a graphical style which has been the envy of other software to the extent that there are now libraries in Python which copy the ggplot2 style!\n\n\n\n\n\nggplot2 introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nBuilding on all of these, the new tmap (Thematic Map) package has changed the game completely and now enables us to read, write and manipulate spatial data and produce visually impressive and interactive maps, very easily. In parallel, the sf (Simple Features) package is helping us re-think the way that spatial data can be stored and manipulated. It’s exciting times for geographic information / spatial data science!"
  },
  {
    "objectID": "01_geographic_data.html#spatial-data-projections",
    "href": "01_geographic_data.html#spatial-data-projections",
    "title": "1  Geographic Information",
    "section": "1.5 Spatial data projections",
    "text": "1.5 Spatial data projections\nSpatial data must be located somewhere on Earth and we need to represent this! We do this with Coordinate Reference Systems, shortened to CRS or often sometimes projections (although a projection is just one part of a coordinate reference system).\nProjections systems are mathematical formulas that specify how our data is represented on a map. These can either be call geographic coordiate reference systems or projected coordinate reference systems. The former treats data as a sphere and the latter as a flat object. You might come across phrases such as a resolution of 5 minutes or a resolution of 30 metres, which can be used to establish what kind of projection system has been used. Let me explain…\nA minute type of resolution (e.g. 5 minute resolution) is a geographic reference system that treats the globe as if it was a sphere divided into 360 equal parts called degrees (which are angular units). Each degree has 60 minutes and each minute has 60 seconds. Arc-seconds of latitude (horizontal lines in the globe figure below) remain almost constant whilst arc-seconds of longitude (vertical lines in the globe figure below) decrease in a trigonometric cosine-based fashion as you move towards the Earth’s poles…\n\n\n\n\n\nLatitude and Longitude. Source: ThoughtCo.\n\n\n\n\nThis causes problems as you increase or decrease latitude the longitudinal lengths alter…For example at the equator (0°, such as Quito) a degree is 111.3 km whereas at 60° (such as Saint Petersburg) a degree is 55.80 km …\nIn contrast a projected coordinate system is defined on a flat, two-dimensional plane (through projecting a spheroid onto a 2D surface) giving it constant lengths, angles and areas…\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\nKnowing this, if we want to conduct analysis locally (e.g. at a national level) or use metric (e.g. kilometres) measurements we need to be able to change the projection of our data or “reproject” it. Most countries and even states have their own projected coordinate reference system such as British National Grid in the above example…Note how the origin (0,0) is has moved from the centre of the Earth to the bottom South West corner of the UK, which has now been ironed (or flattened) out.\n\n\n\n\n\n\nImportant\n\n\n\nProjection rules\nUnits are angular (e.g. degrees, latitude and longitude) or the data is global = Geographic coordinate reference system\nUnits are linear (e.g. feet, metres) or data is at a local level (e.g. national, well the last one is not always true, but likely) = Projected coordinate reference system.\n\n\n\nYou might hear some key words about projections that could terrify you! Let’s break them down:\n\nEllipsoid (or spheroid) = size of shape of the Earth (3d)\nDatum = contains the point relationship (where the origin (0,0) of the map is) between a Cartesian coordinates (flat surface) and Earth’s surface. They can be local or geocentric (see below). They set the origin, the scale and orientation of the Coordiante Reference System (CRS).\nLocal datum = changes the Ellipsoid to align with a certain location on the surface (e.g. BNG that uses the OSGB36 datum). A local datum is anything that isn’t the centre of the Earth.\nGeocentric datum = the centre is equal to the Earth’s centre of gravity (e.g. WGS84).\nGeodetic datum = global datum (see above for datum meaning) for representing features (e.g. points and polygons) on earth\nGeodesy (from which we get Geodetic) = measuring Earth’s shape and features (e.g. gravity field).\nCoordinate reference system (CRS) = Formula that defines how the 2D map (e.g. on your screen or a paper map) relates to the 3D Earth. Sometimes called a spatial Reference System (SRS). It also stores the datum information.\n\n\n\n\n\n\n\nTip\n\n\n\nTake home message\nWhen you do analysis on multiple datasets make sure they are all use the same Coordinate Reference System.\nIf it’s local (e.g. city of country analysis) then use a local projected CRS where possible."
  },
  {
    "objectID": "01_geographic_data.html#data-download",
    "href": "01_geographic_data.html#data-download",
    "title": "1  Geographic Information",
    "section": "1.6 Data download",
    "text": "1.6 Data download\nOk, after all that theory we can start downloading data! In this case we will be joining the “health in general” question from the 2021 census to LSOAs in London (although you could select any area).\nMake a new R project and put this data into a data folder\nThe health data be accessed either from:\n\nThe ONS. Note make sure you have selected the right area (in our case LSOA).\nThe London data store. Note this is excel data and the function we would need is read_excel(excel document, sheet number)\n\nOur spatial data can also be accessed from either:\n\nThe ONS\nThe London data store\n\nIn this example i will use the health data from the ONS and spatial data from the London Datastore.\nFirst, load the packages we need:\n\nlibrary(sf)\nlibrary(tidyverse)\n\nThen our data…\n\n# spatial data\n\nLSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\")\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac1_data\\statistical-gis-boundaries-london\\ESRI\\LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\n# health data\n  \nhealth &lt;- readr::read_csv(\"prac1_data/TS037-2021-3-filtered-2024-01-24T17_28_55Z.csv\")  \n\nRows: 214032 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): Lower layer Super Output Areas Code, Lower layer Super Output Areas...\ndbl (2): General health (6 categories) Code, Observation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTo check our spatial data let’s make a quick map with the thematic maps pacakge (tmap) this works on the grammar of graphics (famous from ggplots), similar to the grammar of data manipulation (tidyverse) it works on a layered approach. Here we specify the dataset and then how we want to sytle it..In the most basic form…\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n# plot or view - view will make it interactive\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\n# load the sf object\ntm_shape(LSOAs) +\n  # style it with polygons.\n  tm_polygons(col = NA, alpha = 0.5)"
  },
  {
    "objectID": "01_geographic_data.html#wrangle",
    "href": "01_geographic_data.html#wrangle",
    "title": "1  Geographic Information",
    "section": "1.7 Wrangle",
    "text": "1.7 Wrangle\nBefore progressing it’s also good practice to standardise our column names…we can do so with the janitor package…\n\n\n\n\n\njanitor::clean_names() example. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\nlibrary(janitor)\n\nLSOAs &lt;- janitor::clean_names(LSOAs)\n\nhealth &lt;- janitor::clean_names(health)\n\nNext we need to join the health data to the spatial data…to do so need to identify a unique column in each dataset to perform the join on, such as a code for the LSOAs (e.g. lsoa11cd and lower layer super output areas code ).\nOnce we have the code we can select a join type…\n\n\n\n\n\nSQL join types. Source: SQL Join Diagram, dofactory\n\n\n\n\nTypically in spatial analysis we use a left join - this retains everything in th left data (which is our spatial data set) and joins data from the right only where there are matches\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\nIf there are multiple matches then a new row is created (e.g. If there were two health rows for a single LSOA)\nIf there are no matches then the data is dropped (e.g. the LSOAs not in London)\n\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\njoined_data &lt;- LSOAs %&gt;%\n  left_join(., \n            health,\n            by = c(\"lsoa11cd\" = \"lower_layer_super_output_areas_code\"))\n\nWarning in sf_column %in% names(g): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 1 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning."
  },
  {
    "objectID": "01_geographic_data.html#long-vs-wide-data",
    "href": "01_geographic_data.html#long-vs-wide-data",
    "title": "1  Geographic Information",
    "section": "1.8 Long vs wide data",
    "text": "1.8 Long vs wide data\nYou will get a warning saying that each row in x (the spatial data) was expected to match just 1 y row. However, our health data is long data (also called tidy data). This differs from “regular” wide data as…\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\n\n\nThis figure is taken directly from Grolemund and Wickham (2017) Chapter 12.Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. Source: KSK analytics\n\n\n\n\nTypically in GIS we need our data messy (or wide) where the variables have their own column and each row is an area.\nTo so do first we must make the data into a tibble..\n\n\n\n\n\nData Object Type and Structure. Source: Exploratory Data Analysis in R, Gimond 2022\n\n\n\n\nWe should reflect on data types in R, which will influence the structure we select. Note a tibble is very similar (the same!) to a dataframe, expect you are provided with additional information when printing.\n\nlibrary(tidyr)\n\njoined_data_wide &lt;- joined_data %&gt;%\n  as_tibble(.)%&gt;%\n  select(lsoa11cd, general_health_6_categories, observation, usualres, hholdres, popden)%&gt;%\n  tidyr::pivot_wider(.,\n#    id_cols=1:8,\n    names_from = general_health_6_categories,\n    values_from = observation)%&gt;%\n    clean_names(.)\n\nWhen we make the tibble we lose the geometry column and so our data becomes non spatial again…really we could have done our wrangling first and then conducted a join! This will create a bit of a mess with columns too (as we already have some), so we will need to select the ones we want…\n\njoined_data_wide_joined &lt;- LSOAs %&gt;%\n  left_join(., \n            joined_data_wide,\n            by = c(\"lsoa11cd\" = \"lsoa11cd\"))%&gt;%\n    select(lsoa11cd, msoa11cd, usualres.x, hholdres.x, popden.x, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health)"
  },
  {
    "objectID": "01_geographic_data.html#map",
    "href": "01_geographic_data.html#map",
    "title": "1  Geographic Information",
    "section": "1.9 Map",
    "text": "1.9 Map\nOnce we have the wide data we can compute other metrics - this is especially important for mapping as we must never map count data, unless the spatial units are the same size (e.g. hexagons). Instead we should normalise our data using some kind of common denominator….for example percent of usual residents with bad health…where the number of usual residents will vary across the spatial units.\n\njoined_data_wide_joined_map &lt;- joined_data_wide_joined%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres.x)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nMake a basic map!\n\n# select the sf object to map\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\")\n\ntm1\n\n\n\n\nThere are some basic issues with our map that we can resolve…\n\nThe legend is covering the data and is using the object name (with underscores)\nNo scale bar\nThe LSOAs are fairly small and so it can be challenging to interpret them\n\n\nlibrary(tmap)\n\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\ntm1\n\n\n\n\nTo export the map…\n\ntmap_save(tm1, 'very bad health.png')\n\nThis hasn’t solved the LSOA issue - whereby the map is challenging to read due to the spatial units used. We can consider aggregating our units to MSOA as that column is provided within the LSOA data…\nTo do so we’d need to:\n\nAggregate our current data\nLoad the MSOA spatial data, then join and map as above.\n\nTo aggregate the data we use a function called group_by() which is always followed by summarise(). Group by places our data into groups based on a selected column (e.g. MSOA) and then summarises the data for each group (e.g. number of people with very bad health)\n\nMSOA_data &lt;- joined_data_wide_joined_map %&gt;%\n  as_tibble(.)%&gt;%\n  select(-lsoa11cd, -geometry, -percent_very_bad)%&gt;%\n  group_by(msoa11cd)%&gt;%\n  summarise_all(sum)\n\nIn the above code select(-variable) means drop that variable, this has allowed me to use the summarise_all() function as opposed to just summarise(). Now each column is aggregated to MSOA!\nCalculate the percentages\n\nMSOA_data_percent &lt;- MSOA_data%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres.x)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nRead in the MSOA spatial data\n\nMSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp\")%&gt;%\n  clean_names(.)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac1_data\\statistical-gis-boundaries-london\\ESRI\\MSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB 1936 / British National Grid\n\n\nJoin…\n\nMSOA_joined &lt;- MSOAs %&gt;%\n  left_join(., \n            MSOA_data_percent,\n            by = c(\"msoa11cd\" = \"msoa11cd\"))%&gt;%\n    select(msoa11cd, msoa11cd, usualres, hholdres, popden, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health, percent_very_bad)\n\nMap the MSOAs\n\ntm2 &lt;- tm_shape(MSOA_joined) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\ntm2\n\n\n\n\nPlot them together…\n\nt=tmap_arrange(tm1, tm2, ncol=2)\n\nt"
  },
  {
    "objectID": "02_raster_reproducibility.html#learning-outcomes",
    "href": "02_raster_reproducibility.html#learning-outcomes",
    "title": "2  Raster and Reproducibility",
    "section": "2.1 Learning outcomes",
    "text": "2.1 Learning outcomes\nBy the end of this practical you should be able to:\n\nLoad, manipulate and interpret raster layers\nExplain the use of and differences between Git and GitHub\nCreate reproducible and open R code\nProduce RMarkdown documents that explain code and analysis"
  },
  {
    "objectID": "02_raster_reproducibility.html#rmarkdown-quarto",
    "href": "02_raster_reproducibility.html#rmarkdown-quarto",
    "title": "2  Raster and Reproducibility",
    "section": "2.2 RMarkdown / Quarto",
    "text": "2.2 RMarkdown / Quarto\nBefore we start the practical content we should now be moving to a type of markdown file as opposed to R scripts. This could be either a Quarto file or R Markdown. Both let you can show code, explanations and results within the same document.\nOften it could be very hard to reproduce results owing to a lack of information in the methodology / user guides or walk through not matching up with the latest version of software. Think back to a time where you had to use software and consult a massive user guide in how to use it. Markdown is a big improvement as it puts all of the information in the same document, which can then be converted into a range of different formats — html for webpages, word documents, PDFs, blogs, books — virtually everything!\nRMarkdown is dependent on R, however the recent development of Quarto has removed the reliance on R being multilingual with R, Python, Javascript and Julia. Quarto is separate software but (i think) is now installed by defaulted with R.\nIn your Rproject go, File &gt; New File &gt; select R Markdown or Quarto.\nIn RMarkdown you will see a “knit” option at the bar just above your code. In Quarto this will be “render”. These will run all code in the entire document and allow us to export our code to other formats - such as html or pdf.\nYou must put your code in a code chunk and these can be added with C+ in the top tool bar. We can control code chunks in different ways….\n\necho=FALSE - code does not appear in the final rendered document but the output does\nmessage=FALSE - messages don’t appear\nwarning=FALSE - warnings don’t appear\neval=FALSE - the code in that chunk does not run.\n\nHeading levels are denoted by #\ne.g. # = level 1, ## level 2 and so on. In the bottom left of your code you will see an orange # which will let you navigate between your headings.\nStart today with an RMarkdown or Quarto file. If you select the latter you can also make use of the visual editor which will make the Quarto file look like a word processor (top left of the document)."
  },
  {
    "objectID": "02_raster_reproducibility.html#intro",
    "href": "02_raster_reproducibility.html#intro",
    "title": "2  Raster and Reproducibility",
    "section": "2.3 Intro",
    "text": "2.3 Intro\nToday we are going to take some pixel level data (raster data) &gt; summarise it per polygon (e.g. country or district) &gt; share out code with the world on GitHub."
  },
  {
    "objectID": "02_raster_reproducibility.html#data",
    "href": "02_raster_reproducibility.html#data",
    "title": "2  Raster and Reproducibility",
    "section": "2.4 Data",
    "text": "2.4 Data\nWe will use data from the Malaria Atlas project - specifically count of deaths from Plasmodium falciparum (malaria). We’ll also get some boundary data from the Humanitarian Data Exchange. Note, we could use the Database of Global Administrative Areas - (GADM), however we would need to do some advanced data wrangling when joining the non-spatial data to the spatial data.\nI will use Nigeria as an example, but feel free to select another country.\n\nNigeria Local Govenrment Area boundaries bondaries. Note, this is the second option - not the file that includes .gbd\ncount of deaths from Plasmodium falciparum (malaria). I have downloaded the whole world, but you can also select just the country.\n2020 population data, in order to normalise our data."
  },
  {
    "objectID": "02_raster_reproducibility.html#data-loading",
    "href": "02_raster_reproducibility.html#data-loading",
    "title": "2  Raster and Reproducibility",
    "section": "2.5 Data loading",
    "text": "2.5 Data loading\nLoad in the population data. We will need to join this data to a spatial layer later on, so check it for a unique identifier.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\npopulation &lt;- read_csv(\"prac2_data/nga_admpop_adm2_2020.csv\")%&gt;%\n  clean_names(.)\n\nNew names:\nRows: 773 Columns: 78\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): ADM0_NAME, ADM0_PCODE, ADM1_NAME, ADM1_PCODE, ADM2_NAME, ADM2_PCODE dbl\n(54): F_TL, M_TL, T_TL, F_00_04, F_05_09, F_10_14, F_15_19, F_20_24, F_2... lgl\n(18): ...61, ...62, ...63, ...64, ...65, ...66, ...67, ...68, ...69, ......\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...63`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...75`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n\n# check the type of column\n\nDatatypelist &lt;- population %&gt;% \n  summarise_all(class) %&gt;%\n  pivot_longer(everything(), \n               names_to=\"All_variables\", \n               values_to=\"Variable_class\")\n\nDatatypelist\n\n# A tibble: 78 × 2\n   All_variables Variable_class\n   &lt;chr&gt;         &lt;chr&gt;         \n 1 adm0_name     character     \n 2 adm0_pcode    character     \n 3 adm1_name     character     \n 4 adm1_pcode    character     \n 5 adm2_name     character     \n 6 adm2_pcode    character     \n 7 f_tl          numeric       \n 8 m_tl          numeric       \n 9 t_tl          numeric       \n10 f_00_04       numeric       \n# … with 68 more rows\n\n\nRaster packages in R have and are still evolving rapidly. In 2023 some key packages will retired like maptools, rgdal and rgeos as their maintainer, Roger Bivand retired. The old raster used sp objects (the precursor to sf) for vector data and also the rgdal package - the Geospatial Data Abstraction Library (GDAL) for reading, writing and converting between spatial formats.\nThe terra package (2020) is somewhat mixed with raster, however the retiring packages are only needed by raster, with the terra package replacing raster. Terra is much faster than raster as datasets that can’t be loaded into RAM are stored on disc and when loaded doesn’t read the values. Then when computations occur the do so in chunks. Terra is very well documented and the functions are very similar to raster - https://rspatial.org/terra/pkg/index.html\nLoad in the raster layer for 2020\n\nlibrary(terra)\n\nterra 1.6.17\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:janitor':\n\n    crosstab\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nmalaria &lt;- rast(\"prac2_data/202206_Global_Pf_Mortality_Count_2000/202206_Global_Pf_Mortality_Count_2020.tif\")\n\n# check the raster info\nmalaria\n\nclass       : SpatRaster \ndimensions  : 3480, 8640, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -60, 85  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : 202206_Global_Pf_Mortality_Count_2020.tif \nname        : 202206_Global_Pf_Mortality_Count_2020 \nmin value   :                                0.0000 \nmax value   :                              481.9917 \n\n\nWe are going to use local government areas. According to Wikipedia, in Nigeria:\n\nThe federal government’s role is mostly limited to coordinating the affairs of the university teaching hospitals, Federal Medical Centres (tertiary healthcare) while the state government manages the various general hospitals (secondary healthcare) and the local government focuses on dispensaries (primary healthcare),[4] which are regulated by the federal government through the NPHCDA.\n\nDespite this let’s calculate which local government area has the most cases…\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.4.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\nLGA &lt;- st_read(\"prac2_data/nga_adm_osgof_20190417/nga_admbnda_adm2_osgof_20190417.shp\")\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac2_data\\nga_adm_osgof_20190417\\nga_admbnda_adm2_osgof_20190417.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nBut before we do…"
  },
  {
    "objectID": "02_raster_reproducibility.html#projection",
    "href": "02_raster_reproducibility.html#projection",
    "title": "2  Raster and Reproducibility",
    "section": "2.6 Projection",
    "text": "2.6 Projection\nWhilst yesterday we saw the theory of coordinate reference systems we didn’t actually change any!\nThe coordinates are stored in the geometry column of your sf object contain the information to enable points, lines or polygons to be drawn on the screen. Rasters are similar but contain the points of the bottom left pixel.\nYou can see in our cases we have:\n\na SpatRaster with dimensions, extent and coord ref of lon/lat WGS 84 (EPSG:4326)\na multipolygon - every point within the polygon will have coordinates that are in a certain reference system, here Geodetic CRS: WGS 84. The geodetic means geographic, if it was a projected coordinate reference system it would say projected CRS\n\nWe need some more information on projections before we can progress….\n\n2.6.1 WGS84\nThe World Geodetic System 84 (WGS84 - last revised in 1984) is one of the most common global projection systems, used in nearly all GPS devices. However there are many versions of WGS84, some geographic and some projected coordinate reference systems.\n3D globe\n\nGPS devices and point data sets often use the Geographic version (EPSG code 4326), usually referred to just as WGS84\nGoogle Earth uses the Geographic version as it is a 3D globe (EPSG code 4326)\n\n\nIn GIS if you add a layer that is 3D (e.g. in WGS84) it will map it on the 2D screen! But how? It takes the latitude and longitude values and maps them as planar coordinates. This means it linearly maps 3D points to a 2D plane….some people might refer to this as “unprojected” but i would say that it has a geographic coordinate reference system.\n\n\n\n\n\nPlanar projections. Source: Wikipedia (2023)\n\n\n\n\n\nFlat\n\nGoogle Maps / Carto / Bing / Open street map use Web Mercator, Google Web Mercator, Spherical Mercator, WGS 84 Web Mercator or WGS 84/Pseudo-Mercator which is a projected coordinate reference system (EPSG code 3857). It is a variant of the Mercator map projection.\nThe WGS 1984 world Mercator projection is a projected coordinate reference system following the Mercator map transformation (EPSG code 3395)\n\nThe projected versions will have units of meters and the geographic versions units of degrees.\nIn the rare occasion some data was collected in a Geographic coordinate reference system other than WGS84, you might find that the points / polygons won’t line up with data in WGS84. This is because different Geographic coordinate reference systems use different ellipsoids (shape / size of Earth) and/or datums (the origin point (0,0), typically they use Earth’s centre of gravity). To make them match you need to transform a dataset. That said QGIS will project ‘on the fly’ meaning if you add layers with a different CRS it will make them appear the same on the screen (changing the projection live or ‘on the fly’). It won’t change the CRS of the data itself though, you have to do that manually.\nAn example of another Geographic coordinate reference systems is the North American 1983 Datum (NAD83) that uses a different ellipsoid (longer by 0.21mm compared to WGS84), it also just has reference points on the North America plate (that moves 1-2cm a year), WGS84 has them all over the world.\nThis same logic applies for projected coordinate references systems that could be different, as they use a different origin and the projected coordinate reference system still contains the geographic one, the projected part tells the computer how to draw the shape.\n\n\n2.6.2 Proj4\nWhilst we were able to identify the CRS of our layer using print another alternative is to find the proj4 string. A proj4 string is meant to be a compact way of identifying a coordinate reference system. Let’s extract ours…\n\nlibrary(sf)\nst_crs(LGA)$proj4string\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n“Well that’s clear as mud!” I hear you cry! Yes, not obvious is it!\n\nThe proj4-string basically tells the computer where on the earth to locate the coordinates that make up the geometries in your file and what distortions to apply (i.e. if to flatten it out completely etc.) It’s composed of a list of parameters separated by a +. Here are projection proj uses latitude and longitude (so it’s a geographic not projected CRS). The datum is WGS84 that uses Earth’s centre mass as the coordinate origin (0,0).\nThe Coordiante systems in R chapter by Gimond (2019) provides much more information on Proj4. However, i’d advise trying to use EPSG codes, which we come onto next.\n\nSometimes you can download data from the web and it doesn’t have a CRS. If any boundary data you download does not have a coordinate reference system attached to it (NA is displayed in the coord. ref section), this is not a huge problem — it can be added afterwards by adding the proj4string to the file or just assigning an EPSG code.\n\nTo find the proj4-strings for a whole range of different geographic projections, use the search facility at http://spatialreference.org/ or http://epsg.io/.\n\n\n2.6.3 EPSG\nNow, if you can store a whole proj4-string in your mind, you must be some kind of savant (why are you doing this course? you could make your fortune as a card-counting poker player or something!). The rest of us need something a little bit more easy to remember and for coordinate reference systems, the saviour is the European Petroleum Survey Group (EPSG) — (naturally!). Now managed and maintained by the International Association of Oil and Gas producers — EPSG codes are short numbers represent all coordinate reference systems in the world and link directly to proj4 strings. We saw these last week in the [Making some maps] section.\nThe EPSG code for the WGS84 World Geodetic System (usually the default CRS for most spatial data) is 4326 — http://epsg.io/4326\n\nIf our data didn’t have a spatial reference system, we could have just set it using st_set_crs()\n\n\nLGA &lt;- LGA %&gt;%\n  st_set_crs(., 4326)\n\nNormally if a layer has a missing CRS, it’s WGS84. But check for any metadata that might list it."
  },
  {
    "objectID": "02_raster_reproducibility.html#reprojecting",
    "href": "02_raster_reproducibility.html#reprojecting",
    "title": "2  Raster and Reproducibility",
    "section": "2.7 Reprojecting",
    "text": "2.7 Reprojecting\nReprojecting your data is something that you might have to (or want to) do, on occasion. Why? Well, one example might be if you want to measure the distance of a line object, or the distance between two polygons. This can be done far more easily in a projected coordinate system (where the units are measured in metres) than it can a geographic coordinate system such as WGS84 (where the units are degrees). It is good practice to select a local coordiante reference system to undertake your analysis in.\nHowever for generating online maps in packages like leaflet, your data will need to be in WGS84, rather than a projected (flat) reference system .\nIn our case the data has been provided in WGS84. Searching epsg.io gives us a variety of options - we are looking for a CRS in meters.\nBefore we reproject our data we should clip (cookie cutter) out our study area.\nNote our data must be in the same CRS when we do spatial operations..\nHere:\n\nCropping, crops the raster to the extent (typically a sqaure around the vector data)\nMasking acts like a cookie cutter.\n\n\nmalaria_nga &lt;- malaria %&gt;% \n  terra::crop(., LGA)%&gt;%\n  terra::mask(., LGA)%&gt;%\n  terra::project(., \"EPSG:26393\")\n\nNow, let’s re-project our vector data\n\nLGA_nga &lt;- LGA %&gt;%\n  st_transform(., 26393)%&gt;%\n    clean_names(.)"
  },
  {
    "objectID": "02_raster_reproducibility.html#data-joining",
    "href": "02_raster_reproducibility.html#data-joining",
    "title": "2  Raster and Reproducibility",
    "section": "2.8 Data joining",
    "text": "2.8 Data joining\nJoin our population data to our vector data using the unique identifier in each of the datasets then select the columns we want to keep for this analysis…\n\njoined_data &lt;- LGA_nga %&gt;%\n  left_join(., \n            population,\n            by = c(\"adm2_pcode\" = \"adm2_pcode\"))%&gt;%\n  select(adm2_pcode, adm2_en, t_tl)"
  },
  {
    "objectID": "02_raster_reproducibility.html#malaria-per-lga",
    "href": "02_raster_reproducibility.html#malaria-per-lga",
    "title": "2  Raster and Reproducibility",
    "section": "2.9 Malaria per LGA",
    "text": "2.9 Malaria per LGA\nTo extract the raster in each polygon terra::extract() will identify the cells that have a centroid (central point) within each polygon. I have used the argument fun=sum() which will then sum all of the cells within each polygon. Note the output length is 774 - the same as our sf polygon data. If we left this argument out it would return all of the cells and the ID’s would be replicated (e.g. the first four cells have the ID of 1 and are within the first polygon). The length of the data frame increases to around 43,000. We could still work out the sum per LGA by using group_by() and summarise() like we say yesterday.\nThis process will return a data frame where the ID reflects the row of the sf object (e.g. ID 1 is the top row of the sf polygon data). For more information of extracting raster data see: https://tmieno2.github.io/R-as-GIS-for-Economists/extracting-values-from-raster-layers-for-vector-data.html\n\nLGA_pop_extract &lt;- malaria_nga %&gt;%\n  terra::extract(., joined_data, fun=sum)\n\nLGA_pop_extract\n\n     ID X202206_Global_Pf_Mortality_Count_2020\n1     1                              81.277229\n2     2                             175.569527\n3     3                              77.099926\n4     4                             459.198448\n5     5                              72.545191\n6     6                             200.915854\n7     7                             820.837324\n8     8                             171.463963\n9     9                             102.928163\n10   10                             825.630615\n11   11                             394.654127\n12   12                            2321.565441\n13   13                             132.196976\n14   14                             446.152905\n15   15                             956.979896\n16   16                             290.444331\n17   17                             226.431078\n18   18                             231.872137\n19   19                             381.323390\n20   20                             144.002135\n21   21                             211.623055\n22   22                             214.422531\n23   23                             677.578377\n24   24                                    NaN\n25   25                             428.386162\n26   26                              68.822559\n27   27                             128.844365\n28   28                             338.164091\n29   29                             229.753050\n30   30                             191.077412\n31   31                              92.343609\n32   32                             512.455879\n33   33                             482.218170\n34   34                             118.799303\n35   35                             354.617079\n36   36                             454.971613\n37   37                             363.327646\n38   38                              95.296338\n39   39                             114.658797\n40   40                              40.389406\n41   41                             157.502551\n42   42                             128.276656\n43   43                             349.537813\n44   44                             206.612995\n45   45                             311.914862\n46   46                             196.762578\n47   47                             483.940969\n48   48                              44.685845\n49   49                            1165.837719\n50   50                             524.672569\n51   51                             346.384310\n52   52                             162.831162\n53   53                             538.635058\n54   54                             607.126980\n55   55                             315.476232\n56   56                             438.658938\n57   57                             128.902108\n58   58                             174.876990\n59   59                             187.974059\n60   60                             352.410950\n61   61                             178.238162\n62   62                             215.401184\n63   63                             239.884507\n64   64                             192.683237\n65   65                             171.469370\n66   66                             603.589075\n67   67                             167.875973\n68   68                             179.556081\n69   69                             187.187938\n70   70                             243.674301\n71   71                             349.325185\n72   72                             324.255466\n73   73                             462.169214\n74   74                              78.792662\n75   75                             145.100417\n76   76                             198.289261\n77   77                             432.811171\n78   78                             162.735638\n79   79                             181.915554\n80   80                             388.723775\n81   81                             309.477064\n82   82                             499.574890\n83   83                             130.396102\n84   84                             458.127232\n85   85                             561.029335\n86   86                               3.440239\n87   87                             419.316846\n88   88                             113.110719\n89   89                             368.466458\n90   90                             428.128950\n91   91                             311.041999\n92   92                             638.509943\n93   93                             664.912186\n94   94                             153.587903\n95   95                             674.718880\n96   96                             254.284164\n97   97                             307.838974\n98   98                             525.830071\n99   99                             367.816299\n100 100                             231.410019\n101 101                             529.557395\n102 102                              95.838760\n103 103                             755.103849\n104 104                             120.001934\n105 105                             793.081831\n106 106                              92.906223\n107 107                             209.259087\n108 108                             222.921044\n109 109                             247.790136\n110 110                             219.485661\n111 111                             424.519493\n112 112                             822.715179\n113 113                             166.546878\n114 114                             167.379667\n115 115                             413.287554\n116 116                             224.010332\n117 117                             238.777819\n118 118                             280.432242\n119 119                             739.757344\n120 120                              53.079895\n121 121                             102.730362\n122 122                             199.391440\n123 123                                    NaN\n124 124                             146.335055\n125 125                             171.217270\n126 126                                    NaN\n127 127                             141.104690\n128 128                             245.377048\n129 129                             324.330322\n130 130                             410.063941\n131 131                             140.577210\n132 132                             104.138044\n133 133                             210.720802\n134 134                             338.186646\n135 135                             683.898751\n136 136                              23.207888\n137 137                              77.344852\n138 138                              93.307850\n139 139                             218.581867\n140 140                             137.306820\n141 141                             928.164802\n142 142                             176.184509\n143 143                              65.700156\n144 144                             376.512259\n145 145                             537.650269\n146 146                             308.994211\n147 147                             306.976735\n148 148                             239.635416\n149 149                             473.358952\n150 150                             322.278965\n151 151                             285.184897\n152 152                             564.523515\n153 153                             161.322343\n154 154                              90.484735\n155 155                             548.067059\n156 156                             780.522011\n157 157                             431.854704\n158 158                             278.295074\n159 159                             164.574609\n160 160                             108.548505\n161 161                             325.399070\n162 162                             327.679993\n163 163                             206.287800\n164 164                             231.234637\n165 165                             102.141041\n166 166                             340.087457\n167 167                             162.838528\n168 168                             317.955412\n169 169                              49.779457\n170 170                             180.447576\n171 171                             229.613176\n172 172                              73.306091\n173 173                             188.780262\n174 174                             331.248496\n175 175                             141.947489\n176 176                            1078.556097\n177 177                             786.453034\n178 178                             490.104492\n179 179                             114.788152\n180 180                             165.021439\n181 181                             562.612328\n182 182                             200.554005\n183 183                             107.285245\n184 184                             101.198884\n185 185                              90.602926\n186 186                             125.528979\n187 187                             307.893929\n188 188                             240.121256\n189 189                             352.241474\n190 190                              84.428730\n191 191                             186.852823\n192 192                              87.893996\n193 193                             312.958795\n194 194                              91.869235\n195 195                              73.408596\n196 196                             453.295514\n197 197                             118.666281\n198 198                              63.833431\n199 199                             159.210622\n200 200                             169.362940\n201 201                             141.721508\n202 202                              82.100881\n203 203                             183.122790\n204 204                             301.221263\n205 205                             184.268805\n206 206                             274.152800\n207 207                             105.068965\n208 208                             142.988698\n209 209                             115.734904\n210 210                              55.652504\n211 211                              92.549165\n212 212                             153.145296\n213 213                             119.900203\n214 214                             325.508611\n215 215                             305.263003\n216 216                             825.213295\n217 217                             228.919003\n218 218                             242.252392\n219 219                             202.665516\n220 220                             287.344572\n221 221                             541.519607\n222 222                             318.549085\n223 223                             257.869852\n224 224                             291.895738\n225 225                             478.579586\n226 226                             333.472383\n227 227                             527.394490\n228 228                             232.383621\n229 229                             129.998987\n230 230                             556.282186\n231 231                             348.004188\n232 232                             191.146958\n233 233                             337.073865\n234 234                             355.613606\n235 235                             238.116100\n236 236                             194.543301\n237 237                             707.668702\n238 238                             659.162076\n239 239                             220.175608\n240 240                             407.678443\n241 241                              87.782395\n242 242                             451.831325\n243 243                             338.804678\n244 244                             143.466098\n245 245                             860.286201\n246 246                             216.395636\n247 247                              21.711898\n248 248                             265.809391\n249 249                             306.094234\n250 250                              68.687658\n251 251                             137.904379\n252 252                             293.919971\n253 253                             261.711145\n254 254                             272.855852\n255 255                             136.408721\n256 256                             228.734702\n257 257                             131.102049\n258 258                             139.587817\n259 259                             277.062446\n260 260                             252.921438\n261 261                              40.285912\n262 262                             306.607465\n263 263                            1059.976918\n264 264                             290.391785\n265 265                             128.783948\n266 266                             261.531776\n267 267                             740.311351\n268 268                             363.560075\n269 269                             247.982680\n270 270                              74.413602\n271 271                             249.576122\n272 272                              21.026999\n273 273                             319.204101\n274 274                             117.734114\n275 275                             188.993057\n276 276                             788.361176\n277 277                             414.356289\n278 278                             133.440567\n279 279                             181.457249\n280 280                             121.320713\n281 281                             252.776783\n282 282                             174.153077\n283 283                             140.912145\n284 284                             156.244828\n285 285                             130.196685\n286 286                             109.711915\n287 287                             147.371617\n288 288                             172.187255\n289 289                              69.310068\n290 290                             319.263992\n291 291                             856.807083\n292 292                             770.270851\n293 293                             621.633419\n294 294                             514.275368\n295 295                             237.322524\n296 296                             116.987420\n297 297                             958.173431\n298 298                              79.853020\n299 299                             532.773455\n300 300                             382.935802\n301 301                            1072.676815\n302 302                              54.323399\n303 303                             436.314840\n304 304                             356.324454\n305 305                              72.820143\n306 306                            1001.279600\n307 307                             995.821948\n308 308                             147.698123\n309 309                             204.841166\n310 310                             382.258913\n311 311                             139.793151\n312 312                              69.084363\n313 313                             574.217236\n314 314                             419.716953\n315 315                             732.992535\n316 316                            1609.059122\n317 317                             391.194284\n318 318                             177.684040\n319 319                             208.125426\n320 320                             218.042563\n321 321                              74.527662\n322 322                             245.635996\n323 323                             189.085434\n324 324                             551.377020\n325 325                             771.525120\n326 326                             216.585617\n327 327                             222.869238\n328 328                             200.765172\n329 329                             154.410472\n330 330                             180.153087\n331 331                             105.935972\n332 332                             368.156093\n333 333                             121.202951\n334 334                              80.888223\n335 335                             324.925840\n336 336                             111.053718\n337 337                             647.769543\n338 338                             532.217811\n339 339                              64.668800\n340 340                             141.415302\n341 341                             578.457541\n342 342                              37.047890\n343 343                             139.593254\n344 344                              71.451099\n345 345                             322.124691\n346 346                             135.470819\n347 347                             296.218204\n348 348                             147.106475\n349 349                             624.739947\n350 350                             238.645128\n351 351                             149.212919\n352 352                             660.623703\n353 353                             212.579143\n354 354                             190.864952\n355 355                             228.281698\n356 356                              80.262501\n357 357                             111.853397\n358 358                             189.869628\n359 359                             335.254297\n360 360                             195.693635\n361 361                             484.969480\n362 362                             227.126051\n363 363                             359.619312\n364 364                             475.569248\n365 365                             817.853134\n366 366                            1087.011864\n367 367                              93.431901\n368 368                             372.853701\n369 369                             495.013333\n370 370                              67.137213\n371 371                             136.119823\n372 372                             974.269867\n373 373                             274.031946\n374 374                             307.701504\n375 375                              48.571754\n376 376                             135.457330\n377 377                             234.502454\n378 378                             216.806492\n379 379                             407.660832\n380 380                             231.942879\n381 381                             326.523392\n382 382                             355.737068\n383 383                             417.042248\n384 384                             271.161443\n385 385                              98.506055\n386 386                             244.376092\n387 387                             396.151558\n388 388                             130.403261\n389 389                             203.249647\n390 390                             260.471018\n391 391                             393.972265\n392 392                             962.738217\n393 393                             139.701746\n394 394                             618.542154\n395 395                             523.191736\n396 396                             139.888412\n397 397                             178.260036\n398 398                             414.418281\n399 399                             518.362576\n400 400                             121.729420\n401 401                             227.821301\n402 402                             337.794399\n403 403                             210.367764\n404 404                             468.600580\n405 405                             572.368248\n406 406                                    NaN\n407 407                              75.572247\n408 408                             239.579837\n409 409                             743.288908\n410 410                             583.110674\n411 411                             407.412795\n412 412                             296.221441\n413 413                            1237.642311\n414 414                              97.743327\n415 415                             518.268487\n416 416                             362.699185\n417 417                             329.039586\n418 418                             247.502477\n419 419                             183.846332\n420 420                             115.531758\n421 421                             337.226379\n422 422                             181.760435\n423 423                             259.904507\n424 424                             291.788455\n425 425                             421.078769\n426 426                             140.698311\n427 427                             194.519078\n428 428                             315.589715\n429 429                              78.704288\n430 430                             343.685447\n431 431                             442.764131\n432 432                             215.851229\n433 433                             240.914819\n434 434                             940.785961\n435 435                             202.769907\n436 436                             189.457961\n437 437                             264.345232\n438 438                             246.971461\n439 439                              19.357319\n440 440                              97.382290\n441 441                             517.615764\n442 442                             669.488583\n443 443                             317.340494\n444 444                             593.838037\n445 445                             370.206059\n446 446                             732.417819\n447 447                             161.463565\n448 448                             938.175266\n449 449                             235.049953\n450 450                             310.469174\n451 451                             176.576954\n452 452                             181.172089\n453 453                             189.136030\n454 454                             692.477976\n455 455                             312.221734\n456 456                             409.200637\n457 457                             275.995716\n458 458                             236.253672\n459 459                            1092.299488\n460 460                             282.195845\n461 461                             125.646923\n462 462                             114.624123\n463 463                             138.030556\n464 464                             643.542826\n465 465                             261.470132\n466 466                             196.750583\n467 467                             323.103208\n468 468                             147.403689\n469 469                             627.385529\n470 470                             213.597295\n471 471                             161.891624\n472 472                              74.253534\n473 473                             107.558783\n474 474                             334.831291\n475 475                              83.123748\n476 476                             995.266391\n477 477                              73.049128\n478 478                             268.384663\n479 479                              25.134092\n480 480                             257.204056\n481 481                             137.905172\n482 482                             264.853642\n483 483                             467.838264\n484 484                             222.286350\n485 485                             255.341977\n486 486                             461.058537\n487 487                            1034.879785\n488 488                             209.228966\n489 489                             220.443449\n490 490                             996.873019\n491 491                             364.478139\n492 492                              95.487783\n493 493                             311.178534\n494 494                             753.215271\n495 495                             254.333851\n496 496                             242.789248\n497 497                             273.339199\n498 498                             586.079401\n499 499                             215.512792\n500 500                             106.818525\n501 501                             150.896410\n502 502                             371.168088\n503 503                             676.122332\n504 504                             381.327983\n505 505                             133.284981\n506 506                             127.815168\n507 507                              60.923250\n508 508                             655.093522\n509 509                              68.961972\n510 510                              41.299557\n511 511                             191.750362\n512 512                              77.255490\n513 513                             105.904092\n514 514                             365.833462\n515 515                             144.814072\n516 516                             153.457748\n517 517                             146.759912\n518 518                             137.769464\n519 519                             235.229214\n520 520                             289.359935\n521 521                             442.588606\n522 522                             103.696104\n523 523                              83.470128\n524 524                              95.005245\n525 525                             167.653145\n526 526                              43.329202\n527 527                                    NaN\n528 528                             556.462543\n529 529                             149.557351\n530 530                             408.756106\n531 531                             310.350731\n532 532                             128.766439\n533 533                             373.813829\n534 534                             227.582851\n535 535                             395.832062\n536 536                             203.499798\n537 537                             651.840370\n538 538                             110.246735\n539 539                              84.981564\n540 540                             106.405848\n541 541                             478.042697\n542 542                             186.129885\n543 543                             789.254608\n544 544                             893.181942\n545 545                             110.436321\n546 546                             416.763504\n547 547                             481.769258\n548 548                             462.416742\n549 549                             160.704503\n550 550                             265.104999\n551 551                             173.575215\n552 552                             638.121864\n553 553                             147.519542\n554 554                             174.733671\n555 555                             787.025575\n556 556                             373.941002\n557 557                             162.659553\n558 558                             472.474228\n559 559                             169.652821\n560 560                              53.572440\n561 561                             184.243012\n562 562                              88.197146\n563 563                             280.144683\n564 564                             600.876729\n565 565                              73.422437\n566 566                             103.139277\n567 567                             159.487114\n568 568                             175.003183\n569 569                             182.014055\n570 570                              65.994678\n571 571                              99.214753\n572 572                             245.973562\n573 573                             169.499241\n574 574                             492.687881\n575 575                             235.136508\n576 576                             205.309276\n577 577                             313.107225\n578 578                             214.614514\n579 579                             254.951116\n580 580                             330.614037\n581 581                             520.566136\n582 582                             105.350511\n583 583                              93.366555\n584 584                             173.538583\n585 585                             802.345535\n586 586                             204.093304\n587 587                             146.789217\n588 588                             235.779198\n589 589                             497.011243\n590 590                             236.489696\n591 591                             148.416563\n592 592                             185.823620\n593 593                             104.687037\n594 594                             138.790782\n595 595                             441.709756\n596 596                             157.020482\n597 597                              55.420670\n598 598                             474.778249\n599 599                             305.241143\n600 600                             539.682734\n601 601                             539.787593\n602 602                              45.305595\n603 603                             148.257576\n604 604                              96.849447\n605 605                             156.963736\n606 606                             391.374311\n607 607                             191.096563\n608 608                             313.434031\n609 609                             268.527596\n610 610                             330.577934\n611 611                             398.447372\n612 612                              98.395302\n613 613                              85.106615\n614 614                             249.037666\n615 615                             292.261051\n616 616                             287.007294\n617 617                             163.352637\n618 618                             326.889386\n619 619                             421.794903\n620 620                             182.734515\n621 621                             144.274717\n622 622                              90.651637\n623 623                             513.416916\n624 624                             288.449244\n625 625                              98.512047\n626 626                             508.920279\n627 627                             520.724746\n628 628                             357.663107\n629 629                             218.206074\n630 630                             214.136499\n631 631                              97.292925\n632 632                             317.948767\n633 633                             194.479533\n634 634                             330.879727\n635 635                              99.903773\n636 636                             165.085068\n637 637                              75.063454\n638 638                             158.132187\n639 639                             145.099666\n640 640                             184.575917\n641 641                             206.533416\n642 642                             777.766382\n643 643                              68.909120\n644 644                             183.104166\n645 645                             105.181213\n646 646                             336.876126\n647 647                             790.879832\n648 648                             306.696569\n649 649                             550.261665\n650 650                             420.624573\n651 651                             351.185856\n652 652                             899.474561\n653 653                             146.039017\n654 654                             405.900654\n655 655                             308.482578\n656 656                             526.142226\n657 657                             950.752603\n658 658                              90.629544\n659 659                             287.871213\n660 660                             468.805935\n661 661                             405.189973\n662 662                             386.987917\n663 663                              67.812388\n664 664                             188.473032\n665 665                             223.942417\n666 666                             454.884567\n667 667                             137.448875\n668 668                             314.027397\n669 669                             310.633202\n670 670                             363.887187\n671 671                             430.885070\n672 672                             287.254294\n673 673                             418.729486\n674 674                             432.418388\n675 675                             605.059276\n676 676                             320.581654\n677 677                             851.395401\n678 678                             101.231406\n679 679                             362.949948\n680 680                             514.459048\n681 681                             157.067857\n682 682                             771.605865\n683 683                             200.498415\n684 684                             725.871776\n685 685                              38.438461\n686 686                              62.623066\n687 687                             238.153553\n688 688                             144.680605\n689 689                             196.032175\n690 690                             131.117392\n691 691                             707.314244\n692 692                             296.532627\n693 693                            1099.369095\n694 694                             318.442615\n695 695                             179.850831\n696 696                             551.209206\n697 697                              83.707161\n698 698                             437.139628\n699 699                             267.398718\n700 700                             169.822797\n701 701                             310.136928\n702 702                             267.253772\n703 703                             109.057724\n704 704                              86.746790\n705 705                              86.096441\n706 706                             184.872942\n707 707                             488.878572\n708 708                             426.084020\n709 709                             259.612216\n710 710                              48.438423\n711 711                             215.069658\n712 712                             579.850883\n713 713                             644.168807\n714 714                             287.439089\n715 715                             268.336092\n716 716                             306.184048\n717 717                              58.242018\n718 718                              45.805399\n719 719                             204.988387\n720 720                             205.766666\n721 721                             189.418569\n722 722                             257.471636\n723 723                              72.658940\n724 724                             333.437701\n725 725                             140.800916\n726 726                             205.540686\n727 727                              61.655753\n728 728                             411.158987\n729 729                             320.713949\n730 730                             349.078899\n731 731                             655.583952\n732 732                             697.373276\n733 733                              80.361626\n734 734                             116.357475\n735 735                             270.616014\n736 736                              91.570865\n737 737                              73.941720\n738 738                              92.850540\n739 739                             211.968336\n740 740                             503.319125\n741 741                             197.659989\n742 742                             212.047510\n743 743                             374.392488\n744 744                             131.949647\n745 745                             203.744653\n746 746                             231.801749\n747 747                             127.061920\n748 748                             559.312266\n749 749                             631.559679\n750 750                             484.051365\n751 751                             315.536680\n752 752                             265.715048\n753 753                             128.385820\n754 754                             110.767039\n755 755                             186.308310\n756 756                             197.301824\n757 757                             115.699436\n758 758                             198.971066\n759 759                             601.063106\n760 760                              70.971352\n761 761                              73.340040\n762 762                              74.798140\n763 763                              61.926450\n764 764                             176.125241\n765 765                             532.711910\n766 766                              79.063233\n767 767                              67.774314\n768 768                             264.474946\n769 769                             125.322354\n770 770                             483.744837\n771 771                             676.702375\n772 772                             374.438545\n773 773                             211.835730\n774 774                             340.596711"
  },
  {
    "objectID": "02_raster_reproducibility.html#final-dataset",
    "href": "02_raster_reproducibility.html#final-dataset",
    "title": "2  Raster and Reproducibility",
    "section": "2.10 Final dataset",
    "text": "2.10 Final dataset\nAdd a unique ID to our joined data (the polygons with the population) then left join again with our malaria counts per\n\njoined_data &lt;- joined_data %&gt;%\n  mutate(id := seq_len(nrow(.)))\n\njoined_data_malaria &lt;- joined_data %&gt;%\n  left_join(., LGA_pop_extract,\n            by = c(\"id\" = \"ID\"))%&gt;%\n  clean_names(.)\n\nNow we have a spatial data set with the total population and malaria count for each LGA."
  },
  {
    "objectID": "02_raster_reproducibility.html#calculations",
    "href": "02_raster_reproducibility.html#calculations",
    "title": "2  Raster and Reproducibility",
    "section": "2.11 Calculations",
    "text": "2.11 Calculations\nCalculate the rate of malaria deaths per 100,000 of each LGA…\n\njoined_data_malaria &lt;- joined_data_malaria %&gt;%\n  mutate(death_rate_100_000 =\n           (x202206_global_pf_mortality_count_2020/t_tl)*100000)\n\nDo not make a map yet! that forms part of the homework task…"
  },
  {
    "objectID": "02_raster_reproducibility.html#git-github-rmarkdown",
    "href": "02_raster_reproducibility.html#git-github-rmarkdown",
    "title": "2  Raster and Reproducibility",
    "section": "2.12 Git, GitHub, RMarkdown",
    "text": "2.12 Git, GitHub, RMarkdown\nHere you will learn how to produce work that is open, reproducible, share able and portable using RStudio, RMarkdown, Git and GitHub. As more and more researchers and organisations publish associated code with their manuscripts or documents it’s very important to become adept at using these tools.\nThe tools you will use are:\n\nRStudio is a graphical user interface (that you should already be familiar with) — it contains a number of features which make it excellent for authoring reproducible and open geographic data science work.\nRMarkdown is a version of the Markdown markup language which enables plain text to be formatted to contain links to data, code to run, text to explain what you a producing and metadata to tell your software what kinds of outputs to generate from your markdown code. For more information on RMarkdown look here.\nGit is a software version control system which allows you to keep track of the code you produce and the changes that you or others make to it.\nGitHub is an online repository that allows anyone to view the code you have produced (in whatever language you choose to program in) and use/scrutinise/contribute to/comment on it.\n\n\n2.12.1 Setup\n\nFirst need to install Git — https://git-scm.com/\nGo to http://github.com, create an account and create a new repository (call it anything you like - ‘gis_code’ or something similar), making sure it is public and you check the box that says ‘initialise new repository with a README’ — click ‘create repository’ at the bottom\n\n\n\n\n\n\n\n\n\n\n\nYour new repository (‘repo’) will be created and this is where you will be able to store your code online. You will notice that a README.md markdown file has also been created. This can be edited to tell people what they are likely to find in this repository.\n\n\n\n2.12.2 Using RStudio with Git\nIn summer 2021 GitHub changed it’s authentication process to a token based system as opposed to a password based system. David Keys provided an excellent overview with some videos that documented this change and how to now set things up, which i have adapted here\n\n2.12.2.1 Check Git is installed\nNext to the console window you will see a terminal tab, check Git is installed with which git then git --version you should get a message in response that says where your git installation is and the version you have.\n\n\n2.12.2.2 Configure your Git\nYou need to tell Git who you are and your GitHub username. The easiest way is to use the usethis package, you will need to install and library it.\nThen, in the console, type the function edit_git_config()\nA Git config will load and you need to change your name and email to match GitHub.\nIf this is empty use the following template and save the file.\n\n[user]\n    name = \n    email = \n    username = \n\n\n\n2.12.2.3 Connect Git to GitHub\nFrom GitHub you need to generate a personal access token. You can use the function create_github_token() from the usethis package or also through\n\nGitHub &gt; settings &gt; Developer settings &gt; personal access tokens &gt; generate new token.\n\nUse a descriptive name and consider saving the token - it won’t save on GitHub\nThe last step is to store this token in Git with the gitcreds package &gt; install and load it &gt; then use the function gitcreds_set() &gt; copy your token in.\nThere are many different ways to use Git, GitHub and R. I will focus to two here. Read the next two parts then decicde which to use.\n\n\n\n2.12.3 Way 1 - starting fresh\nThe first way assumes you are starting fresh - meaning you don’t have a project but you know you want to put your work online.\n\nUnder [Set up your GitHub] we made a repository on GitHub. Copy that URL.\nOpen RStudio &gt; File New Project &gt; Version Control &gt; Git\nCopy in the repository URL and provide a project directory name…but it should populate when you paste in the URL\n\n\n\n\n\n\n\n\n\n\n\n\n2.12.4 Way 2 - If have an existing project\n\nOpen RStudio and your existing project (or make a new one…i will make one here). In RStudio Tools &gt; Global Options, under ‘Git/SVN’ check the box to allow version control and locate the folder on your computer where the git.exe file is located — if you have installed git then this should be automatically there. If you make a new project make sure you create a file (.R or .Rmd through File &gt; New File), add something to it, then save it (File &gt; Save As) into your project folder. When it saves it should appear in the bottom right Files window.\nNext go Tools &gt; Project Options &gt; Git/SVN &gt; and select the version control system as Git. You should now see a git tab in the environment window of RStudio (top right) and the files also appear under the Git tab. It should look something like this….\n\n\n\n\n\n\n\n\n\n\nNow you will be able to use Git and GitHub as per the following instructions…\n\n\n2.12.5 Commiting to Git\n\nAs well as saving (as you normally do with any file), which saves a copy to our local directory, we will also ‘commit’ or create a save point for our work on git.\nTo do this, you should click the ‘Git’ icon and up will pop a menu like the one below:\n\n\n\n\n\n\n\n\n\n\nYou can also click the Git tab that will have appeared in the top-right window of RStudio. Up will then pop another window that looks a little like the one below:\n\n\n\n\n\n\n\n\n\n\nStage the changes, add a commit message so you can monitor the changes you make, then click commit\nMake some more changes to your file and save it. Click commit again then in the review changes box you will be able to see what has changed within your file. Add a commit message and click commit:\n\n\n\n\n\n\n\n\n\n\n\n\n2.12.6 Push to Github\nWe need to create a new GitHub repo for our local project. Luckily the usethis package can do this for us. Simply type the function use_github() in the console and a new GitHub repo will appear using the name of your project!\nNow we can push our changes to GitHub using the up arrow either in the RStudio Git tab (environment quadrant), or from the review changes box (opens when you click commit).\nBut….if the push button is greyed out go to the section [Greyed out push button]\n\n\n2.12.7 Pull from GitHub\n\nPull will take any changes to the global repo and bring them into your local repo. Go to your example GitHub repo (online) and click on your test file &gt; edit this file.\nAdd a line of code or a comment, preview the changes then commit directly to the main branch.\n\n\n\n\n\n\n\n\n\n\n\nNow in RStudio click the down arrow (Pull) request. Your file should update in RStudio.\n\nIf you were to update your file on GitHub and your local one in RStudio separately you would receive an error message in RStudio when you attempted to commit.\nIf you’d rather use terminal to control Git then you can. If you have a large project RStudio sometimes has limits on file name length (e.g. this might occur with a book, like this one). To get around this you can use the following commands:\n\ngit add . to stage all files\ngit commit -m \"commit comment\" to commit all the staged files\ngit push to push the committed files to the remote\n\nNow take the code you have used this week and put it on GitHub and add the link to the shared spreadsheet\nHowever note that GitHub has a maximum file size of 50mb, if you have larger files you must ignore them in the .gitignore file…e.g. my .gitignore looks like this..\n\nprac1_data/*\nprac2_data/*\nprac3_data/*\nprac4_data/*\nprac1_images/*\nprac2_images/*\nprac3_images/*\nprac4_images/*\n\n\n\n2.12.8 Forking\nA Fork in GitHub is a copy of someone’s repository to your own GitHub account. You could use it as a base starting point for your project or to make a fix and then submit a pull request to the original owner who would then pull your changes to their repository.\n\n\n\n\n\n\nImportant\n\n\n\nYour homework task is to:\n\nselect a GitHub respoitoiry from the shared spreadsheet\nclick fork (on GitHub)\nclone the repository into a new Rproject (as per way 1 above)\nmake a map of the malaria death rate\npush to GitHub\nsubmit a pull request to the original owner (otherwise know as the original upstream repository)…\n\nthe pull request will look something like this…\n\n\n\n\n\nPull request. Source: codex (2024)\n\n\n\n\n\n\nGit and GitHub have many more features that we haven’t covered today. This is because when we are working on our own on a small project we hopefully won’t need them.\nFor more information on these features - such as going back in time from Git or GitHub see my other resources"
  },
  {
    "objectID": "03_point_patterns.html#learning-outcomes",
    "href": "03_point_patterns.html#learning-outcomes",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.1 Learning outcomes",
    "text": "3.1 Learning outcomes\nBy the end of this practical you should be able to:\n\nDescribe and evaluate methods for analysing spatial patterns\nExecute data cleaning and manipulation appropriate for analysis\nDetermine the locations of spatial clusters using point pattern analysis methods\nInvestigate the degree to which values at spatial points are similar (or different) to each other\nInterpret the meaning of spatial autocorrleation in spatial data"
  },
  {
    "objectID": "03_point_patterns.html#introduction",
    "href": "03_point_patterns.html#introduction",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.2 Introduction",
    "text": "3.2 Introduction\nToday you will learn how to begin to analyse patterns in spatial data with points and spatially continuous observations.\nThe questions we want to answer are:\n\nFor any given London Ward, are Pharmacies distributed randomly or do they exhibit some kind of dispersed or clustered pattern\nAre the values (in this case the density of pharmacies) similar (or dissimilar) across the wards of London."
  },
  {
    "objectID": "03_point_patterns.html#data",
    "href": "03_point_patterns.html#data",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.3 Data",
    "text": "3.3 Data\nLoad our packages\n\nlibrary(spatstat)\n\nLoading required package: spatstat.data\n\n\nLoading required package: spatstat.geom\n\n\nspatstat.geom 2.4-0\n\n\nLoading required package: spatstat.random\n\n\nspatstat.random 2.2-0\n\n\nLoading required package: spatstat.core\n\n\nLoading required package: nlme\n\n\nLoading required package: rpart\n\n\nspatstat.core 2.4-4\n\n\nLoading required package: spatstat.linnet\n\n\nspatstat.linnet 2.3-2\n\n\n\nspatstat 2.3-4       (nickname: 'Watch this space') \nFor an introduction to spatstat, type 'beginner' \n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::collapse() masks nlme::collapse()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(here)\n\nhere() starts at C:/Users/Andy/OneDrive - University College London/Teaching/Guest/Oxford_24/Spatial analysis of public health data\n\nlibrary(sp)\n\nWarning: package 'sp' was built under R version 4.2.3\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.4.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(stringr)\nlibrary(spdep)\n\nLoading required package: spData\n\nlibrary(RColorBrewer)\n\nGet the pharmacy data: https://datashare.ed.ac.uk/handle/10283/2501 and load it\n\nPharmacy &lt;- st_read(\"prac3_data/PharmacyEW/PharmacyEW.shp\")%&gt;%\n  #remove any duplicates\n  distinct()\n\nReading layer `PharmacyEW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac3_data\\PharmacyEW\\PharmacyEW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 10589 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 137070 ymin: 19187 xmax: 655138 ymax: 653253\nProjected CRS: OSGB 1936 / British National Grid\n\n#check CRS\n#st_crs(Pharmacy)\n\n#check with a plot\ntm_shape(Pharmacy) +\n  tm_dots(col = \"blue\")\n\n\n\n\nGet the London ward data and load it https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london\n\nwards&lt;- st_read(\"prac3_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Ward_CityMerged' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac3_data\\statistical-gis-boundaries-london\\statistical-gis-boundaries-london\\ESRI\\London_Ward_CityMerged.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 625 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB 1936 / British National Grid\n\n\nCheck the data\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA)\n\n\n\n\n\n3.3.1 Wrangle data\nAs we can see above the pharmacy data is for the whole of the UK and we are just interested in London. So, we need to spatially subset our points within our study area…\nHere, the second operator is blank , , - this controls which attributes are kept, although I’d rather keep all of them and manipulate with the tidyverse.\n\nPharmacysub &lt;- Pharmacy[wards, , op=st_within]\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA, alpha=0.5)+\ntm_shape(Pharmacysub) +\n  tm_dots(col=\"blue\")\n\n\n\n\nWhen we spatial subset data like this there are different topological relations we can specify. The default is intersects, but we could also use BluePlaquesSub &lt;- BluePlaques[BoroughMap, , op = st_within], with the operator or op set to st_within, to identify points completely within the borough outline, or a variety of other options such as st_overlaps, st_touches, st_contains, st_disjoint. Any possible topological relationship you can think of a function will exist for it…visually this looks like…where each tick denotes the relations that apply to the polygons. Note, that in several cases multiple topological relations would work.\n\n\n\n\n\nTopological relations between vector geometries. Source: Lovelace et al. 2022\n\n\n\n\nWe can also just use the function which will have the indices of where they intersect.\n\n# add sparse=false to get the complete matrix.\nintersect_indices &lt;-st_intersects(wards, Pharmacy)\n\nIf you have used a graphic user interface GIS before, this is the same as select by location (e.g. select by location in QGIS), and as using filter from dplyr is the same as select by attribute.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the difference between intersects and within for points like ours?"
  },
  {
    "objectID": "03_point_patterns.html#point-patterns",
    "href": "03_point_patterns.html#point-patterns",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.4 Point patterns",
    "text": "3.4 Point patterns\nFor point pattern analysis we need a point pattern object (ppp)…within an observation window. This is specific to the spatstat package as we can’t do this with sf, SpatialPolygonsDataFrames or SpatialPointsDataFrames.\nFor this example i will set the boundary to London so we can see the points\n\nboroughs &lt;- st_read(\"prac3_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac3_data\\statistical-gis-boundaries-london\\statistical-gis-boundaries-london\\ESRI\\London_Borough_Excluding_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB 1936 / British National Grid\n\n#now set a window as the borough boundary\nwindow &lt;- as.owin(boroughs)\nplot(window)\n\n\n\n#create a sp object\nPharmacysubSP&lt;- Pharmacysub %&gt;%\n  as(., 'Spatial')\n#create a ppp object\nPharmacysubSP.ppp &lt;- ppp(x=PharmacysubSP@coords[,1],\n                          y=PharmacysubSP@coords[,2],\n                          window=window)\n\nWarning: data contain duplicated points\n\nPharmacysubSP.ppp %&gt;%\n  plot(.,pch=16,cex=0.5, \n              main=\"Pharmacies in London\")\n\n\n\n\n\n3.4.1 Quadrat analysis\nSo as you saw in the lecture, we are interesting in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR. That’s different from a CRS! Be careful!\nThe most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the quadrat count() function in spatstat. Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…\n\n#First plot the points\n\nplot(PharmacysubSP.ppp,\n     pch=16,\n     cex=0.5, \n     main=\"Pharmacies\")\n\n#now count the points in that fall in a 20 x 20\n#must be run all together otherwise there is a plot issue\n\nPharmacysubSP.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20)%&gt;%\n    plot(., add=T, col=\"red\")\n\n\n\n\nWe want to know whether or not there is any kind of spatial patterning associated with pharmacies in areas of London. If you recall from the lecture, this means comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distribution, based on the Poisson distribution.\nUsing the same quadratcount() function again (for the same sized grid) we can save the results into a table:\n\n#run the quadrat count\nQcount &lt;- PharmacysubSP.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::count(Var1=Freq)%&gt;%\n  dplyr::rename(Freqquadratcount=n)\n\nOK, so we now have a frequency table — next we need to calculate our expected values. The formula for calculating expected probabilities based on the Poisson distribution is:\n\\[Pr= (X =k) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!}\\] where:\n\nx is the number of occurrences\nλ is the mean number of occurrences\ne is a constant- 2.718\n\n\nsums &lt;- Qcount %&gt;%\n  #calculate the total blue plaques (Var * Freq)\n  mutate(total = Var1 * Freqquadratcount) %&gt;%\n  # then the sums\n  dplyr::summarise(across(everything(), sum))%&gt;%\n  dplyr::select(-Var1) \n\nlambda&lt;- Qcount%&gt;%\n  #calculate lambda - sum of freq count / sum of total plaques\n  mutate(total = Var1 * Freqquadratcount)%&gt;%\n  dplyr::summarise(across(everything(), sum)) %&gt;%\n  mutate(lambda=total/Freqquadratcount) %&gt;%\n  dplyr::select(lambda)%&gt;%\n  pull(lambda)\n\nCalculate expected using the Poisson formula from above \\(k\\) is the number of pharmacies counted in a square and is found in the first column of our table…\n\nQCountTable &lt;- Qcount %&gt;%\n  #Probability of number of plaques in quadrant using the formula \n  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%&gt;%\n  #now calculate the expected counts based on our total number of plaques\n  #and save them to the table\n  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))\n\nPlot them\n\nQCountTable_long &lt;- QCountTable %&gt;% \n  pivot_longer(c(\"Freqquadratcount\", \"Expected\"), \n               names_to=\"countvs_expected\", \n               values_to=\"value\")\n\nggplot(QCountTable_long, aes(Var1, value)) +\n  geom_line(aes(colour = countvs_expected ))\n\n\n\n\nCheck for association between two categorical variables - we are looking to see if our freqneucy is similar to the expected (which is random)\nTo check for sure, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).\nA Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.\nIf the p-value of our Chi-Squared test is &lt; 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p &gt; 0.05. If our p-value is &gt; 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is &lt; 0.05, this indicates that we do have clustering in our points.\n\nteststats &lt;- quadrat.test(PharmacysubSP.ppp, nx = 20, ny = 20)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\nChi square with a p value &lt; 0.05 therefore some clustering…but from the plot, this was expected\n\n\n3.4.2 Ripley K\nOne way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radii. This is what Ripley’s K function computes. We can conduct a Ripley’s K test on our data very simply with the spatstat package using the kest() function.\nRipley’s K is defined as…\n\\[K(r) = \\lambda^{-1} \\sum{i}\\sum{j}\\frac{I(d_ij&lt;r)}{n}\\]\n\nIn English: Ripley’s K value for any circle radius \\(r\\) =\n\nThe average density of points for the entire study region (of all locations) \\(\\lambda = (n/ \\Pi r^2))\\)\nMultiplied by the sum of the distances \\(d_ij\\) between all points within that search radius, see Dixon page 2 and Amgad et al. 2015\nDivided by the total number of points, n\nI = 1 or 0 depending if \\(d_ij &lt; r\\)\n\n\nThe plot for K has a number of elements that are worth explaining. First, the Kpois(r) line in Red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness. The Black line is the estimated values of K accounting for the effects of the edge of the study area.\nHere, the correction specifies how points towards the edge are dealt with, in this case, border means that points towards the edge are ignored for the calculation but are included for the central points. Section 2.1, here explains the different options.\nWhere the value of K falls above the line, the data appear to be clustered at that distance. Where the value of K is below the line, the data are dispersed…\n\nK &lt;- PharmacysubSP.ppp %&gt;%\n  Kest(., correction=\"border\") %&gt;%\n  plot()\n\n\n\n\nThis was sort of expected too due to our previous analysis - suggesting that there is clustering throughout the points.\n\n\n3.4.3 DBSCAN\nQuadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE in our area of interest the clusters are occurring. To discover this we need to use alternative techniques. One popular technique for discovering clusters in space (be this physical space or variable space) is DBSCAN. For the complete overview of the DBSCAN algorithm, read the original paper by Ester et al. (1996) or consult the wikipedia page\nDBSCAN requires you to input two parameters: 1. Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster\nWe could use the output of Ripley’s K to inform Epsilon or alternatively we can use kNNdistplot() from the dbscan package to find a suitable eps value based on the ‘knee’ in the plot…For MinPts we typically start with 4. But, try increasing the value of k in kNNdistplot you will notice as you increase it the knee becomes less obvious.\n\nIf we have MinPts too low we have a massive cluster\nIf we have MinPts too high we have a small single cluster\n\n\n#first extract the points from the spatial points data frame\nPharmacysub_coords &lt;- Pharmacysub %&gt;%\n  st_coordinates(.)%&gt;%\n  as.data.frame()\n\nPharmacysub_coords%&gt;%\n  dbscan::kNNdistplot(.,k=20)\n\n\n\n\nI started with an eps of 1600 and a minpts of 20..however…the large eps means that the city centre has a massive cluster…this isn’t exactly what i wanted to pull out. Instead i want to identify local clusters of pharmacies so try reducing the eps to 500 and the minpts to 5\nOPTICS will let us remove the eps parameter but running every possible value, however, minpts is always meant to be domain knowledge - see https://stats.stackexchange.com/questions/88872/a-routine-to-choose-eps-and-minpts-for-dbscan\nDepending on your points it might be possible to filter the values you aren’t interested in - this isn’t the case here, but for example stop and search data or flytipping could be filtered (well, depending on the extra data within the columns)\n\n#now run the dbscan analysis\ndb &lt;- Pharmacysub_coords %&gt;%\n  fpc::dbscan(.,eps = 500, MinPts = 5)\n\n#now plot the results\nplot(db, Pharmacysub_coords, main = \"DBSCAN Output\", frame = F)\nplot(wards$geometry, add=T)\n\n\n\n\nOur new db object contains lots of info including the cluster each set of point coordinates belongs to, whether the point is a seed point or a border point etc. We can get a summary by just calling the object. We can now add this cluster membership info back into our dataframe\n\nPharmacysub_coords&lt;- Pharmacysub_coords %&gt;%\n  mutate(dbcluster=db$cluster)\n\nNow create a ggplot2 object from our data\n\nPharmacysub_coordsgt0 &lt;- Pharmacysub_coords %&gt;%\n  filter(dbcluster&gt;0)\n\ndbplot &lt;- ggplot(data=wards)+\n  geom_sf()+\n  geom_point(data=Pharmacysub_coordsgt0, \n                 aes(X,Y, colour=dbcluster, fill=dbcluster))\n#add the points in\n\ndbplot + theme_bw() + coord_sf()\n\n\n\n\nNow, this identifies where we have clustering based on our criteria but it doesn’t show where we have similar densities of pharmacies."
  },
  {
    "objectID": "03_point_patterns.html#spatial-autocorrelation",
    "href": "03_point_patterns.html#spatial-autocorrelation",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.5 Spatial Autocorrelation",
    "text": "3.5 Spatial Autocorrelation\nIn this section we are going to explore patterns of spatially referenced continuous observations using various measures of spatial autocorrelation. Spatial autocorrelation is a measure of similarity between nearby data. We need to add all the points to the London wards then compute a density per ward could also use population here too! or some other data that can give us meaningful comparisons for our variable of interest.\n\n3.5.1 Wrangle data\nTo do so we need to use a spatial join!\nThis is similar to the the joins (e.g. left joins) we explored with attribute data but here we just want to join datasets together based on their geometry and keep all their attribute data, this is useful in the code below where i want to join the pharmacies to the LSOA data\nThe output will be a massive dataset where each pharmacy will be a new row and will retain the attributes of the pharmacy data but also append the attribute of the LSOA st_join() defaults to a left join, so in this case the LSOA data is the left dataset and all the right data has been appended to it. If the left data (LSOA) had no matches (so no pharmacies) it would still appear in the final dataset. The default argument for this is st_intersects but we could also use other topological relationship functions such as st_within() instead…\n\nexample&lt;-st_intersects(wards, Pharmacysub)\n\nexample\n\nSparse geometry binary predicate list of length 625, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: 61, 65\n 3: 53, 59\n 4: 45, 54, 68\n 5: 44, 48, 49, 62\n 6: (empty)\n 7: 60, 66\n 8: 57\n 9: 55\n 10: 43, 56\n\n\nHere the polygon with the ID of 7 Chessington North and Hook has two pharmacies within it…we can check this with st_join (or using QGIS by opening the data).\nBut note the ID column added is different to the ID of the data…open Pharmacysub from the environment window and you will see the IDs that were returned in st_intersects(). The new IDs above take the data and apply 1 to n, where n is the end of the data. So if we subset our pharmacies on row 60 and 66 it will match our result here.\n\ncheck_example &lt;- wards%&gt;%\n  st_join(Pharmacysub)%&gt;%\n  filter(GSS_CODE==\"E05000404\")\n\ncheck_example\n\nSimple feature collection with 2 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 517153.7 ymin: 164037.4 xmax: 519553 ymax: 165447.1\nProjected CRS: OSGB 1936 / British National Grid\n                          NAME  GSS_CODE HECTARES NONLD_AREA LB_GSS_CD\n7   Chessington North and Hook E05000404   192.98          0 E09000021\n7.1 Chessington North and Hook E05000404   192.98          0 E09000021\n                 BOROUGH POLY_ID  ID      PCTName         NamePharm\n7   Kingston upon Thames   50530 130 Kingston PCT Alliance Pharmacy\n7.1 Kingston upon Thames   50530 136 Kingston PCT Alliance Pharmacy\n           Address1    Address2    Address3 Address4 PostCode LSOAName Easting\n7   11 North Parade Chessington      Surrey     &lt;NA&gt;  KT9 1QL     &lt;NA&gt;  518483\n7.1 4 Arcade Parade        Hook Chessington   Surrey  KT9 1AB     &lt;NA&gt;  518015\n    Northing                       geometry\n7     164147 POLYGON ((517175.3 164077.3...\n7.1   164508 POLYGON ((517175.3 164077.3...\n\n\nNow we just take the length of each list per polygon and add this as new column…\n\npoints_sf_joined &lt;- wards%&gt;%\n  mutate(n = lengths(st_intersects(., Pharmacysub)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow map density\n\npoints_sf_joined&lt;- points_sf_joined %&gt;%                    \n  group_by(gss_code) %&gt;%         \n  summarise(density = first(density),\n          name  = first(gss_code))\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density\",\n        style=\"jenks\",\n        palette=\"PuOr\",\n        midpoint=NA)\n\n\n\n\nSo, from the map, it looks as though we might have some clustering of pharmacies in the centre of London and a few other places, so let’s check this with Moran’s I and some other statistics.\nAs we saw in the session we need to create a spatial weight matrix…to do so we need centroid points first…to then compute the neighbours of each centroid…\n\ncoordsW &lt;- points_sf_joined%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nWarning in st_centroid.sf(.): st_centroid assumes attributes are constant over\ngeometries of x\n\n#check, alpha is transparency \ntm_shape(points_sf_joined) +\n    tm_polygons(alpha=0.1)+\ntm_shape(coordsW) +\n  tm_dots(col = \"blue\")\n\n\n\n\n\n\n3.5.2 Weight matrix\nNow we need to generate a spatial weights matrix (remember from the lecture). We’ll start with a simple binary matrix of queen’s case neighbours (otherwise known as Contiguity edges corners). This method means that polygons with a shared edge or a corner will be included in computations for the target polygon…A spatial weight matrix represents the spatial element of our data, this means we are trying to conceptualize and model how parts of the data are linked (or not linked) to each other spatially, using rules that we will set.\nIf the features share a boundary they are contiguous, this can also be classed as only common boundaries — a rook (like chess a rook can move forwards or side wards) or any point in common (e.g. corners / other boundaries) — a queen (like chess a queen can move forwards, backwards or on a diagonal).\nAlternatively instead of using contiguous relationships you can use distance based relationships. This is frequently done with k nearest neighbours in which k is set to the closest observations. e.g. K=3 means the three closest observations.\nIn the first instance we must create a neighbours list — which is a list of all the neighbours. To do so we will use poly2nb() with the argument queen=T saying we want a to use Queens case. Let’s see a summary of the output\n\n#create a neighbours list\nLWard_nb &lt;- points_sf_joined %&gt;%\n  poly2nb(., queen=T)\n\nHave a look at the summary of neighbours - average is 5.88\n\nsummary(LWard_nb)\n\nNeighbour list object:\nNumber of regions: 625 \nNumber of nonzero links: 3680 \nPercentage nonzero weights: 0.94208 \nAverage number of links: 5.888 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12 \n  1   4  15  72 162 182 112  55  14   4   2   2 \n1 least connected region:\n380 with 1 link\n2 most connected regions:\n313 612 with 12 links\n\n\nplot them - we can’t use tmap as it isn’t class of sf, starts, spatial raster not spatraster…\n\nplot(LWard_nb, st_geometry(coordsW), col=\"red\")\n#add a map underneath\nplot(points_sf_joined$geometry, add=T)\n\n\n\n\nNext take our weight list and make it into a matrix …style here denotes the weight type:\n\nB is the basic binary coding (1/0)\nW is row standardised (sums over all links to n)\nC is globally standardised (sums over all links to n)\nU is equal to C divided by the number of neighbours (sums over all links to unity)\nS is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\n\n\n#create a spatial weights matrix from these weights\nLward.lw &lt;- LWard_nb %&gt;%\n  nb2mat(., style=\"W\",  zero.policy=TRUE)\n\nI have used row…based on the lecture what should the value of the weights sum to?\n\nsum(Lward.lw)\n\n[1] 625\n\n\n\n\n3.5.3 Moran’s I\nMoran’s I requires a spatial weight list type object as opposed to matrix, this is simply…\n\nLward.lw &lt;- LWard_nb %&gt;%\n  nb2listw(., style=\"W\",  zero.policy=TRUE)\n\nNow let’s run Moran’s I - test tells us whether the values at neighbouring sites are similar to the target site (giving a Moran’s I close to 1) or the value of the taget is different to the neighbours (close to -1)\n\nI_LWard_Global_Density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  moran.test(., Lward.lw, zero.policy = TRUE)\n\n\n\n3.5.4 Geary’s C\nGeary’s C - tells us whether similar values or dissimilar values are clustering - Geary’s C falls between 0 and 2; 1 means no spatial autocorrelation, &lt;1 - positive spatial autocorrelation or similar values clustering, &gt;1 - negative spatial autocorreation or dissimilar values clustering)\n\nC_LWard_Global_Density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  geary.test(., Lward.lw, zero.policy = TRUE)\n\n\n\n3.5.5 Getis Ord General G\nGetis Ord General G…? This tells us whether high or low values are clustering. If G &gt; Expected = High values clustering; if G &lt; expected = low values clustering\n\nG_LWard_Global_Density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  globalG.test(., Lward.lw, zero.policy = TRUE)\n\nWarning in globalG.test(., Lward.lw, zero.policy = TRUE): Binary weights\nrecommended (sepecially for distance bands)\n\n\nBased on the results write down what you can conclude here…."
  },
  {
    "objectID": "03_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "href": "03_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.6 Local Indicies of Spatial Autocorrelation",
    "text": "3.6 Local Indicies of Spatial Autocorrelation\n\n3.6.1 Moran’s I\nWe can now also calculate local versions of the Moran’s I statistic (for each Ward) and a Getis Ord (G_{i}^{}) statistic to see where* we have hot-spots…\nLocal Moran’s I is:\n\nThe difference between a value and neighbours * the sum of differences between neighbours and the mean\nWhere the the difference between a value and neighbours is divided by the standard deviation (how much values in neighbourhood vary about the mean)\n\nIt returns several columns, of most interest is the Z score. A Z-score is how many standard deviations a value is away (above or below) from the mean. This allows us to state if our value is significantly different than expected value at this location considering the neighours.\nWe are comparing our value of Moran’s I to that of an expected value (computed from a separate equation that uses the spatial weight matrix, and therefore considers the neighbouring values). We are expecting our value of Moran’s I to be in the middle of the distribution of the expected values. These expected values follow a normal distribution, with the middle part representing complete spatial randomness. This is typically between &lt; -1.65 or &gt; +1.65 standard deviations from the mean\nThe null hypothesis is always there is complete spatial randomness. A null hypothesis means:\n\nno statistical significance exists in a set of given observations\n\nIf our value is towards the tails of the distribution then it is unlikely that the value is completely spatially random and we can reject the null hypothesis…as it is not what we expect at this location.\nIn the example where we use a z-score of &gt;2.58 or &lt;-2.58 we interpret this as…\n…&gt; 2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level…this means there is a &lt;1% chance that autocorrelation is not present\nThe Global vs location spatial autocorrelation resource goes through the specific formulas here, but the most important parts are knowing\n\nWhat we are comparing values to in Local Moran’s I\nWhat the results mean\nWhy the results could be important\n\n\nI_LWard_Local_Density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localmoran(., Lward.lw, zero.policy = TRUE)%&gt;%\n  as_tibble()\n\nThis outputs a table, so we need to append that back to our sf of the wards to make a map…\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n    mutate(density_I =as.numeric(I_LWard_Local_Density$Ii))%&gt;%\n    mutate(density_Iz =as.numeric(I_LWard_Local_Density$Z.Ii))%&gt;%\n    mutate(p =as.numeric(I_LWard_Local_Density$`Pr(z != E(Ii))`))\n\nWe’ll set the breaks manually based on the rule that data points:&gt;2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level (&lt;1% chance that autocorrelation not present); &gt;1.96 - &lt;2.58 or &lt;-1.96 to &gt;-2.58 standard deviations are significant at the 95% level (&lt;5% change that autocorrelation not present). &gt;1.65 = 90% etc…like we saw in the lecture…\n\nbreaks1&lt;-c(-1,-0.5,0,0.5,1)\n\nbreaks2&lt;-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)\n\nNow create a new diverging colour brewer palette and reverse the order using rev() (reverse) so higher values correspond to red - see https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\n\nMoranColours&lt;- rev(brewer.pal(8, \"RdGy\"))\n\nRemember Moran’s I - test tells us whether we have clustered values (close to 1) or dispersed values (close to -1) of similar values based on the spatial weight matrix that identifies the neighoburs\nBut… the z-score shows were this is unlikely because of complete spatial randomness and so we have spatial clustering (either clustering of similar or dissimilar values depending on the Moran’s I value) within these locations…\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_I\",\n        style=\"fixed\",\n        breaks=breaks1,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I\")\n\nWarning: Values have found that are less than the lowest break\n\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_Iz\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I Z-score\")\n\n\n\n\n\n\n3.6.2 Getis Ord (G_{i}^{*})\nWhat about the Getis Ord (G_{i}^{*}) statistic for hot and cold spots…\nThis is a very similar concept to Local Moran’s I except it just returns a z-score…remember that a z-score shows how many standard deviations a value (our value) is away (above or below) from the mean (of the expected values)\nUltimately a z-score is defined as:\n\\[Z = \\frac{x-\\mu}{\\sigma}\\] Where:\n\n\\(x\\) = the observed value\n\\(\\mu\\) = the mean of the sample\n\\(\\sigma\\) = standard deviation of sample\n\nNote, consult the Global vs location spatial autocorrelation resource for how this is computed in Local Moran’s I if you are interested, although interpretation is the most important part here.\nHowever, in the case of Getis Ord (G_{i}^{*}) this is the local sum (of the neighbourhood) compared to the sum of all features\nIn Moran’s I this is just the value of the spatial unit (e.g. polygon of the ward) compared to the neighbouring units.\nHere, to be significant (or a hot spot) we will have a high value surrounded by high values. The local sum of these values will be different to the expected sum (think of this as all the values in the area) then where this difference is large we can consider it to be not by chance…\nThe same z-score criteria then applies as before..\nThis summary from L3 Harris nicely summaries the Getis Ord (G_{i}^{*}) output…\n\nThe result of Getis Ord (G_{i}^{*}) analysis is an array of Z-scores, one for each pixel [or polygon], which is the number of standard deviations that the pixel [or polygon] and its neighbors are from the global mean. High Z-scores indicate more intense clustering of high pixel values, indicating hot spots. Low Z-scores indicate more intense clustering of low values, indicating cold spots. Individual pixels with high or low values by themselves might be interesting but not necessarily significant.\n\n\nGi_LWard_Local_Density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localG(., Lward.lw)\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n  mutate(density_G = as.numeric(Gi_LWard_Local_Density))\n\nNote that because of the differences in Moran’s I and Getis Ord (G_{i}^{*}) there will be differences between polyogons that are classed as significant.\nAnd map the outputs…\n\nGIColours&lt;- rev(brewer.pal(8, \"RdBu\"))\n\n#now plot on an interactive map\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_G\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=GIColours,\n        midpoint=NA,\n        title=\"Gi* Z score\")"
  },
  {
    "objectID": "03_point_patterns.html#note",
    "href": "03_point_patterns.html#note",
    "title": "3  Point patterns and autocorrelation",
    "section": "3.7 Note",
    "text": "3.7 Note\nIn the analysis you might see a Moran plot where the values of our variable (density of pharmacies) and plotted against (on the y axis) the spatially lagged version (the average value of the same attribute at neighboring locations). However this plot below shows the value of density in relation to the spatial weight matrix…\nThis is useful as we can express the level of spatial association of each observation with its neighboring ones. Points in the upper right (or high-high) and lower left (or low-low) quadrants indicate positive spatial association of values that are higher and lower than the sample mean, respectively. The lower right (or high-low) and upper left (or low-high) quadrants include observations that exhibit negative spatial association; that is, these observed values carry little similarity to their neighboring ones. Source: https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_variogram_details31.htm#:~:text=The%20Moran%20scatter%20plot%20(Anselin,known%20as%20the%20response%20axis.\n\nMoran_plot_LWard_Global_Density &lt;- points_sf_joined %&gt;%\n  pull(density)%&gt;%\n  as.vector()%&gt;%\n  moran.plot(., Lward.lw)\n\n\n\n\nWhen you see Moran’s I out in the wild you will come across maps with:\n\nhigh values surrounded by high values (HH)\nlow values nearby other low values (LL)\nlow values among high values (LH)\nhigh values among low values (HL)\n\nHere, we use the values we have, of density and Moran’s I, compared to the mean of density and Moran’s I (termed centering). Where the:\n\nvalue of density is greater than 0 and the value of Moran’s I is greater than 0 then high values (of density) are surrounded by other high values (from Moran’s I)= HH\nvalue of density is lower than 0 and the value of Moran’s I is lower than 0 then low values (of density) are surrounded by other low values (from Moran’s I) = LL\nvalue of density is lower than 0 and the value of Moran’s I is higher than 0 then low values (of density) are surrounded by high values (from Moran’s I) = LH\nvalue of density is higher than 0 and the value of Moran’s I is lower than 0 then high values (of density) are surrounded by high values (from Moran’s I) =HL\n\n\nsignif &lt;- 0.1\n\n\n# centers the variable of interest around its mean\npoints_sf_joined2 &lt;- points_sf_joined %&gt;%\n  mutate(mean_density = density- mean(density))%&gt;%\n  mutate(mean_density = as.vector(mean_density))%&gt;%\n  mutate(mean_densityI= density_I - mean(density_I))%&gt;%\n  mutate(quadrant = case_when(mean_density&gt;0 & mean_densityI &gt;0 ~ 4,\n         mean_density&lt;0 & mean_densityI &lt;0 ~ 1,\n         mean_density&lt;0 & mean_densityI &gt;0 ~ 2,\n         mean_density&gt;0 & mean_densityI &lt;0 ~ 3))%&gt;%\n  mutate(quadrant=case_when(p &gt; signif ~ 0, TRUE ~ quadrant))\n\nbrks &lt;- c(0,1,2,3,4,5)\ncolors &lt;- c(\"white\",\"blue\",\"skyblue\",\"pink\",\"red\")\n\n\ntm_shape(points_sf_joined2) +\n    tm_polygons(\"quadrant\",\n        style=\"fixed\",\n        breaks=brks,\n        labels = c(\"insignificant\",\"low-low\",\"low-high\",\"high-low\",\"high-high\"),\n        palette=colors,\n        title=\"Moran's I HH etc\")\n\n\n\n\nSource: https://rpubs.com/quarcs-lab/spatial-autocorrelation\nThis might seem somewhat confusing as if we look in the South East we have low values of Getis Ord (G_{i}^{*}) yet we have shown that these are low (density) and high Moran’s I. But as Matthew Peeples concisely summarises remember…\n\nMoran’s I is a measure of the degree to which the value at a target site is similar to values at adjacent sites. Moran’s I is large and positive when the value for a given target (or for all locations in the global case) is similar to adjacent values and negative when the value at a target is dissimilar to adjacent values.\nGetis Ord \\(G_{i}^{*}\\) identifies areas where high or low values cluster in space. It is high where the sum of values within a neighborhood of a given radius or configuration is high relative to the global average and negative where the sum of values within a neighborhood are small relative to the global average and approaches 0 at intermediate values.\n\nSo here we have a high Moran’s I as the values around it are similar (probably all low) but a low Getis Ord as the values within the local area are low relative to the global average.\n\n3.7.1 Consider what this all means\n\nDo you think Pharmacies take into account the ward they are in when they open?\nDo you think people only go to pharmacies within their ward?\nPharmacies don’t exhibit complete spatial randomness and there seems to be clustering in certain locations….\nAnother question we could move onto is now are pharmacies locating around a need (e.g. health outcomes) and does that mean some of the population in London have poor access or choice.\nFor example….we could now look to see if there is clustering of “Deaths from causes considered preventable, under 75 years, standardised mortality ratio” and if that aligns with (or can be explained by) access / clusters of pharmacies.\nIf you wanted to do this (e.g. see if the ratio was clustered), the data is in a somewhat unfriendly format so you’d need to:\n\nDownload the data for ward from here: https://fingertips.phe.org.uk/profile/local-health/data#page/9/gid/1938133184/ati/8/iid/93227/age/1/sex/4/cat/-1/ctp/-1/yrr/5/cid/4/tbm/1\nRead the Indicator definitions for the last row, specifically column G which tells us how it’s created.\nFilter the data based on the indicator value of 93480 (from the Indicator definitions)\nFilter out the London wards - as we did here and join it to our spatial feature.\nRun a Moran’s I or some other spatial autocorrelation\nJoin this back to our spatial feature that has the Moran’s I for pharmacy density\nDecide what to plot - does dispersion of pharmacies occur in the same spatial units as clustering of deaths considered preventable? This could simply be two maps and a table.\n\nShould it be a requirement to have access…well if we consult the “2022 Pharmacy Access Scheme: guidance” then yes it appears so but…“Pharmacies in areas with dense provision of pharmacies remain excluded from the scheme”\n\n\n\n3.7.2 Extensions\n\nThe use of OPTICS\nWe have spatial autocorrelation now can we try and model local differences in density of pharmacies - i.e. what factors might (or might not) explain the difference here — https://andrewmaclachlan.github.io/CASA0005repo/explaining-spatial-patterns.html.\nOr perhaps does the distance to pharmacies assist in explaining deaths from causes considered preventable, under 75 years. Similarly, we could even use this as a dummy variable (yes/no the areas is / is not in a cluster of pharmacies) in a regression model."
  },
  {
    "objectID": "04_spatial_models.html#introduction",
    "href": "04_spatial_models.html#introduction",
    "title": "4  Spatial models",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this practical you will be introduced to a suite of different models that will allow you to test a variety of research questions and hypotheses through modelling the associations between two or more spatially referenced variables.\nIn 2023 New Zealand scrapped an anti-smoking law that would mean a generational smoking bad for anyone born after 2008. So, here we will explore the factors that might affect smoking in New Zealand and see if they vary spatially.\nThis practical will walk you through the common steps that you should go through when building a regression model using spatial data to test a stated research hypothesis; from carrying out some descriptive visualisation and summary statistics, to interpreting the results and using the outputs of the model to inform your next steps."
  },
  {
    "objectID": "04_spatial_models.html#data",
    "href": "04_spatial_models.html#data",
    "title": "4  Spatial models",
    "section": "4.2 Data",
    "text": "4.2 Data\nThis is the same has homework 1….\n\nGo to the New Zealand spatial data portal and download the file Territorial Authority 2018 (generalised), these are city or district councils. Make sure it’s the Territorial Authority 2018 data not SA1.\nGo to the Stats NZ website and download the Statistical area 1 dataset for 2018 for the whole of New Zealand. Download the csv this time\n\nThe figure below explains the geographies of New Zealand, we could replicate this analysis for SA1 or SA2 data.\n\n\n\nNZ geographies\n\n\n\n4.2.1 Loading\n\nlibrary(sf)\n\nLinking to GEOS 3.9.1, GDAL 3.4.3, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nta &lt;- st_read(\"prac4_data/statsnzterritorial-authority-2018-generalised-SHP/territorial-authority-2018-generalised.shp\")\n\nReading layer `territorial-authority-2018-generalised' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac4_data\\statsnzterritorial-authority-2018-generalised-SHP\\territorial-authority-2018-generalised.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 68 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1067061 ymin: 4701317 xmax: 2523320 ymax: 6242140\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\ncensus &lt;- read_csv(\"prac4_data/SA1_census/Individual_part2_totalNZ-wide_format_updated_16-7-20.csv\")\n\nRows: 32521 Columns: 350\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (313): Area_code_and_description, Area_code, Area_description, Census_20...\ndbl  (37): Census_2006_Religious_affiliation_total_response_Total_CURP, Cens...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n4.2.2 Wrangle\nThe .csv we have loaded contains all the data from the week 1 excel homework in a wide format - that means all the SA1s, SA2s and so are are a new row making a huge data file. Normally i would suggest a cool data sciency way to pull out our Territorial Authorities, however i can’t see anything that would work. For example, we could try and detect part of a string like “City” or “District” however they appear in other geographies.\nSo, until i can think or a better way we are resigned to subsetting by rows!\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ncensus_authorities &lt;- census %&gt;%\n  slice(32413:32480) %&gt;%\n  clean_names()\n\n# or\n\n#census_authorities &lt;- census[32413:32480,]%&gt;%\n#  clean_names()\n\nNow in this case the data can be a little hard to read - we have over 350 columns! It might be sensible here to have a look at the excel document we downloaded in the week 1 homework to identify our our variables.\nLet’s pull out the following variables from the 2018 data:\n\nregular smoker\n\nthat will be modeled by:\n\nmedian income\nhome ownership\nno qualification, highschool certificate\nwe’ll also try and include some spatial data (e.g. density of tobacco shops) at the very end…\n\nHowever, because we are using spatial units and they may differ in size we should normalise our count data (everything but median income which is continous).\nTo do so i will use the “total” column for each variable, although you could also use population count\n\ncensus_authorities_data &lt;- census_authorities %&gt;%\n  select(area_code,\n         area_description,\n         census_2018_smoking_status_01_regular_smoker_curp_15years_and_over,\n         census_2018_smoking_status_total_stated_curp_15years_and_over,\n         census_2018_total_personal_income_median_curp_15years_and_over,\n         census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over,\n         census_2018_individual_home_ownership_total_stated_curp_15years_and_over,\n         census_2018_highest_qualification_000_no_qualification_curp_15years_and_over,\n         census_2018_highest_qualification_total_stated_curp_15years_and_over\n         )\n\nLet’s check the type of our data…\n\nDatatypelist &lt;- census_authorities_data %&gt;% \n  summarise_all(class) %&gt;%\n  pivot_longer(everything(), \n               names_to=\"All_variables\", \n               values_to=\"Variable_class\")\n\nDatatypelist\n\n# A tibble: 9 × 2\n  All_variables                                                          Varia…¹\n  &lt;chr&gt;                                                                  &lt;chr&gt;  \n1 area_code                                                              charac…\n2 area_description                                                       charac…\n3 census_2018_smoking_status_01_regular_smoker_curp_15years_and_over     charac…\n4 census_2018_smoking_status_total_stated_curp_15years_and_over          charac…\n5 census_2018_total_personal_income_median_curp_15years_and_over         charac…\n6 census_2018_individual_home_ownership_02_own_or_partly_own_curp_15yea… charac…\n7 census_2018_individual_home_ownership_total_stated_curp_15years_and_o… charac…\n8 census_2018_highest_qualification_000_no_qualification_curp_15years_a… charac…\n9 census_2018_highest_qualification_total_stated_curp_15years_and_over   charac…\n# … with abbreviated variable name ¹​Variable_class\n\n\nCharacter! we will need to change that…\n\ncensus_authorities_data &lt;- census_authorities_data %&gt;%\n  mutate_at(c(\"census_2018_smoking_status_01_regular_smoker_curp_15years_and_over\",\n        \"census_2018_smoking_status_total_stated_curp_15years_and_over\",\n         \"census_2018_total_personal_income_median_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_total_stated_curp_15years_and_over\",\n         \"census_2018_highest_qualification_000_no_qualification_curp_15years_and_over\",\n         \"census_2018_highest_qualification_total_stated_curp_15years_and_over\"),\n        as.numeric)\n\nThe data is selected and now numeric… let’s normalise…and select what we need\n\ncensus_authorities_data_norm &lt;- census_authorities_data%&gt;%\n  mutate(percent_regular_smoker=\n           (census_2018_smoking_status_01_regular_smoker_curp_15years_and_over/census_2018_smoking_status_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_home_owner=\n           (census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over/census_2018_individual_home_ownership_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_no_qualification=\n           (census_2018_highest_qualification_000_no_qualification_curp_15years_and_over/ census_2018_highest_qualification_total_stated_curp_15years_and_over)*100)%&gt;%\n  select(area_code,\n         area_description,\n         percent_regular_smoker,\n         percent_home_owner,\n         percent_no_qualification,\n         census_2018_total_personal_income_median_curp_15years_and_over\n         )\n\nQuick plot…\n\nlibrary(ggplot2)\n\nq &lt;- qplot(x = percent_no_qualification, \n           y = percent_regular_smoker, \n           data=census_authorities_data_norm)\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\nq + stat_smooth(method=\"lm\", se=FALSE, size=1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere, I have plotted the percentage of regular smokers per territorial area against another variable in the dataset that I think might be influential…percentage with no qualification\nRemember that my null hypothesis would be that there is no relationship between percentage of regular smokers and percentage with no qualification. If this null hypothesis was true, then I would not expect to see any pattern in the cloud of points plotted above.\nAs it is, the scatter plot shows that, generally, as the \\(x\\) axis independent variable (percentage with no qualification) goes up, our \\(y\\) axis dependent variable (percent of smokers) goes up. This is not a random cloud of points, but something that indicates there could be a relationship here and so I might be looking to reject my null hypothesis.\nSome conventions - In a regression equation, the dependent variable is always labelled \\(y\\) and shown on the \\(y\\) axis of a graph, the predictor or independent variable(s) is(are) always shown on the \\(x\\) axis.\nI have added a blue line of best-fit - this is the line that can be drawn by minimising the sum of the squared differences between the line and the residuals. The residuals are all of the dots not falling exactly on the blue line. An algorithm known as ‘ordinary least squares’ (OLS) is used to draw this line and it simply tries a selection of different lines until the sum of the squared differences between all of the residuals and the blue line is minimised, leaving the final solution.\nAs a general rule, the better the blue line is at summarising the relationship between \\(y\\) and \\(x\\), the better the model.\nThe equation for the blue line in the graph above can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\]\nwhere:\n\\(\\beta_0\\) is the intercept (the value of \\(y\\) when \\(x = 0\\) - somewhere around 4 on the graph above);\n\\(\\beta_1\\) is sometimes referred to as the ‘slope’ parameter and is simply the change in the value of \\(y\\) for a 1 unit change in the value of \\(x\\) (the slope of the blue line) - reading the graph above, the change in the value of \\(y\\) reading between 20 and 15 on the \\(x\\) axis looks to be around a change from 15 to 17.5 on the \\(y\\) - about 0.5 per 1 unit value of \\(x\\).\n\\(\\epsilon_i\\) is a random error term (positive or negative) that should sum to 0 - essentially, if you add all of the vertical differences between the blue line and all of the residuals, it should sum to 0.\nAny value of \\(y\\) along the blue line can be modeled using the corresponding value of \\(x\\) and these parameter values. Examining the graph above we would expect the percent smoking in an area where 26% of the population to not have any qualifications, to equal around 17%, but we can confirm this by plugging the \\(\\beta\\) parameter values and the value of \\(x\\) into equation (1):\n\n4 + (0.5*26) + 0\n\n[1] 17"
  },
  {
    "objectID": "04_spatial_models.html#regression",
    "href": "04_spatial_models.html#regression",
    "title": "4  Spatial models",
    "section": "4.3 Regression",
    "text": "4.3 Regression\nIn the graph above, I used a method called ‘lm’ in the stat_smooth() function in ggplot2 to draw the regression line. ‘lm’ stands for ‘linear model’ and is a standard function in R for running linear regression models. Use the help system to find out more about lm - ?lm\nBelow is the code that could be used to draw the blue line in our scatter plot. Note, the tilde ~ symbol means “is modeled by”.\n\n#now model\nmodel1 &lt;- census_authorities_data_norm %&gt;%\n  lm(percent_regular_smoker ~\n       percent_no_qualification,\n     data=.)\n\nLet’s have a closer look at our model…\n\n#show the summary of those outputs\nsummary(model1)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification, \n    data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7961 -2.3344 -0.5826  1.1691 16.4704 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               1.20258    1.98667   0.605    0.547    \npercent_no_qualification  0.64634    0.08289   7.798  6.1e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.588 on 66 degrees of freedom\nMultiple R-squared:  0.4795,    Adjusted R-squared:  0.4716 \nF-statistic:  60.8 on 1 and 66 DF,  p-value: 6.097e-11\n\n\n\n4.3.1 Interpretation\nIn running a regression model, we are effectively trying to test (disprove) our null hypothesis. If our null hypothesis was true, then we would expect our coefficients to = 0.\nIn the output summary of the model above, there are a number of features you should pay attention to:\nCoefficient Estimates - these are the \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) parameter estimates from Equation 1. You will notice that at \\(\\beta_0 = 1.2\\) and \\(\\beta_1 = -0.64\\) they are pretty close (not awful!) to the estimates of 4 and 0.5 that we read from the graph earlier, but more precise.\nCoefficient Standard Errors - these represent the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for a 1% increase in percent with no qualifications, while the model says we might expect GSCE scores to drop by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.\nNote that is the coefficient represents a one unit change, here it is %, as the variable is % unauthorized absence in school So one unit is a 1% change…\nCoefficient t-value - this is the value of the coefficient divided by the standard error and so can be thought of as a kind of standardised coefficient value. The larger (either positive or negative) the value the greater the relative effect that particular independent variable is having on the dependent variable (this is perhaps more useful when we have several independent variables in the model) .\nCoefficient p-value - Pr(&gt;|t|) - the p-value is a measure of significance. There is lots of debate about p-values which I won’t go into here, but essentially it refers to the probability of getting a coefficient as large as the one observed in a set of random data. p-values can be thought of as percentages, so if we have a p-value of 0.5, then there is a 5% chance that our coefficient could have occurred in some random data, or put another way, a 95% chance that out coefficient could have only occurred in our data.\nAs a rule of thumb, the smaller the p-value, the more significant that variable is in the story and the smaller the chance that the relationship being observed is just random. Generally, statisticians use 5% or 0.05 as the acceptable cut-off for statistical significance - anything greater than that we should be a little sceptical about.\nIn r the codes ***, **, **, . are used to indicate significance. We generally want at least a single * next to our coefficient for it to be worth considering.\nR-Squared - This can be thought of as an indication of how good your model is - a measure of ‘goodness-of-fit’ (of which there are a number of others). \\(r^2\\) is quite an intuitive measure of fit as it ranges between 0 and 1 and can be thought of as the % of variation in the dependent variable (in our case percentage of smokers) explained by variation in the independent variable(s). In our example, an \\(r^2\\) value of 0.47 indicates that around 47% of the variation in percentage of smokers can be explained by variation in percentage with no qualifications. In other words, this is quite a good model. The \\(r^2\\) value will increase as more independent explanatory variables are added into the model, so where this might be an issue, the adjusted r-squared value can be used to account for this affect.\n\n\n4.3.2 Assumptions Underpinning Linear Regression\n\n4.3.2.1 Assumption 1 - There is a linear relationship between the dependent and independent variables\nThe best way to test for this assumption is to plot a scatter plot similar to the one created earlier. It may not always be practical to create a series of scatter plots, so one quick way to check that a linear relationship is probable is to look at the frequency distributions of the variables. If they are normally distributed, then there is a good chance that if the two variables are in some way correlated, this will be a linear relationship.\nFor example, look at the frequency distributions of our two variables earlier:\n\n#let's check the distribution of these variables first\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_regular_smoker)) + \n  geom_histogram(aes(y = ..density..),\n                 binwidth = 1) + \n  geom_density(colour=\"red\", \n               size=1, \n               adjust=1)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHere, adding ..density.. means that the histogram is a density plot, this plots the chance that any value in the data is equal to that value.\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_home_owner)) +\n  geom_histogram(aes(y = ..density..),\n                 binwidth = 1) + \n  geom_density(colour=\"red\",\n               size=1, \n               adjust=1)\n\n\n\n\nIt is not a requirement of regression to have normally distributed variables, however if they aren’t the residuals may well be normally distributed, but could vary (meaning we have hetroscedasticity) which we discuss later on.\nOne way that we might be able to achieve a linear relationship between our two variables is to transform the non-normally distributed variable so that it is more normally distributed. For more information on this see transforming variables.\nHowever you will be changing the relationship of your data - it won’t be linear anymore! This could improve your model but is at the expense of interpretation. Aside from log transformation which has the rules in the link above.\nTypically if you do a power transformation you can keep the direction of the relationship (positive or negative) and the t-value will give an idea of the importance of the variable in the model - that’s about it!\n\n\n4.3.2.2 Assumption 2 - The residuals in your model should be normally distributed\nThis assumption is easy to check. When we ran our Model1 earlier, one of the outputs stored in our Model 1 object is the residual value for each case (authority) in your dataset. We can access these values using augment() from broom which will add model output to the original data…\nWe can plot these as a histogram and see if there is a normal distribution:\n\nlibrary(broom)\n\n#save the residuals into your dataframe\nmodel_data &lt;- model1 %&gt;%\n  augment(., census_authorities_data_norm)\n\n#plot residuals\nmodel_data%&gt;%\ndplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  qplot()+ \n  geom_histogram() \n\n\n\n\nExamining the histogram above, i am happy this is rather normal although there is evidently an outlier in our data somewhere.\n\n\n4.3.2.3 Assumption 3 - No Multicolinearity in the independent variables\nNow, the regression model we have be experimenting with so far is a simple bivariate (two variable) model. One of the nice things about regression modelling is while we can only easily visualise linear relationships in a two (or maximum 3) dimension scatter plot, mathematically, we can have as many dimensions / variables as we like.\nAs such, we could extend model 1 into a multiple regression model by adding some more explanatory variables that we think could affect the percentage of people smoking…\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9633 -1.6181 -0.4151  1.3204 12.1627 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     1.762e+01\npercent_no_qualification                                        7.757e-01\npercent_home_owner                                             -3.992e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.922e-05\n                                                               Std. Error\n(Intercept)                                                     5.089e+00\npercent_no_qualification                                        9.198e-02\npercent_home_owner                                              7.015e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  9.961e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      3.463 0.000959\npercent_no_qualification                                         8.433 5.57e-12\npercent_home_owner                                              -5.690 3.40e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -0.394 0.695119\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.96 on 64 degrees of freedom\nMultiple R-squared:  0.6564,    Adjusted R-squared:  0.6403 \nF-statistic: 40.75 on 3 and 64 DF,  p-value: 7.514e-15\n\n\nExamining the output above, it is clear that including these variables into our model improves the fit from an \\(r^2\\) of around 47% to an \\(r^2\\) of 65%. However, income is not significant so we should consider removing it..\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner ,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner, data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6020 -1.6175 -0.3927  1.2883 11.7795 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              16.02096    3.04321   5.264 1.70e-06 ***\npercent_no_qualification  0.79755    0.07283  10.950  &lt; 2e-16 ***\npercent_home_owner       -0.40092    0.06955  -5.764 2.45e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 65 degrees of freedom\nMultiple R-squared:  0.6556,    Adjusted R-squared:  0.645 \nF-statistic: 61.86 on 2 and 65 DF,  p-value: 9.037e-16\n\n\nHere we see the \\(r^2\\) is almost the same. But do our two explanatory variables satisfy the no-multicoliniarity assumption? If not and the variables are highly correlated, then we are effectively double counting the influence of these variables and overstating their explanatory power\n\n\n\n\n\nWord anatomy of multicollinearity. Source: What the Heck is Multicollinearity?, Andrew Ozbun\n\n\n\n\nTo check this, we can compute the product moment correlation coefficient between the variables, using the corrr() package, that’s part of tidymodels. In an ideal world, we would be looking for something less than a 0.8 correlation\n\nlibrary(corrr)\n\nCorrelation &lt;- census_authorities_data_norm %&gt;%\n  dplyr::select(percent_no_qualification, \n               percent_home_owner)%&gt;%\n    correlate()\n\n#visualise the correlation matrix\nrplot(Correlation)\n\n\n\nCorrelation\n\n# A tibble: 2 × 3\n  term                     percent_no_qualification percent_home_owner\n  &lt;chr&gt;                                       &lt;dbl&gt;              &lt;dbl&gt;\n1 percent_no_qualification                   NA                  0.360\n2 percent_home_owner                          0.360             NA    \n\n\nAnother way that we can check for Multicolinearity is to examine the VIF for the model. If we have VIF values for any variable exceeding 10, then we may need to worry and perhaps remove that variable from the analysis:\n\nlibrary(car)\nvif(model2)\n\npercent_no_qualification       percent_home_owner \n                1.149067                 1.149067 \n\n\n\n\n4.3.2.4 Assumption 4 - Homoscedasticity\nHomoscedasticity means that the errors/residuals in the model exhibit constant / homogeneous variance, if they don’t, then we would say that there is hetroscedasticity present. Why does this matter? Andy Field does a much better job of explaining this in discovering statistics — but essentially, if your errors do not have constant variance, then your parameter estimates could be wrong, as could the estimates of their significance.\nThe best way to check for homo/hetroscedasticity is to plot the residuals in the model against the predicted values. We are looking for a cloud of points with no apparent patterning to them.\n\n#print some model diagnositcs. \npar(mfrow=c(2,2))    #plot to 2 by 2 array\nplot(model2)\n\n\n\n\nIn the series of plots above, the first plot (residuals vs fitted), we would hope to find a random cloud of points with a straight horizontal red line. Looking at the plot, the curved red line would suggest some hetroscedasticity, but the cloud looks quite random. Similarly we are looking for a random cloud of points with no apparent patterning or shape in the third plot of standardised residuals vs fitted values. Here, the cloud of points also looks fairly random, with perhaps some shaping indicated by the red line.\nSection 5.7.6 in Tidyverse Skills for Data Science explains each of these plots in more detail\nIn the plots here we are looking for:\n\nResiduals vs Fitted: a flat and horizontal line. This is looking at the linear relationship assumption between our variables\nNormal Q-Q: all points falling on the line. This checks if the residuals (observed minus predicted) are normally distributed\nScale vs Location: flat and horizontal line, with randomly spaced points. This is the homoscedasticity (errors/residuals in the model exhibit constant / homogeneous variance). Are the residuals (also called errors) spread equally along all of the data.\nResiduals vs Leverage - Identifies outliers (or influential observations), the three largest outliers are identified with values in the plot.\n\nThe University of Viginia Library provides examples of good and bad models in relation to these plots.\nThere is an easier way to produce this plot using check_model() from the performance package, that even includes what you are looking for…note that the Posterior predictive check is the comparison between the fitted model and the observed data.\nThe default argument is check=all but we can specify what to check for…see the arguments section in the documentation…e.g. check = c(\"vif\", \"qq\")\n\nlibrary(performance)\n\ncheck_model(model2, check=\"all\")\n\n\n\n\n\n\n4.3.2.5 Assumption 5 - Independence of Errors\nThis assumption simply states that the residual values (errors) in your model must not be correlated in any way. If they are, then they exhibit autocorrelation which suggests that something might be going on in the background that we have not sufficiently accounted for in our model.\n\n4.3.2.5.1 Standard autocorrelation\nIf you are running a regression model on data that do not have explicit space or time dimensions, then the standard test for autocorrelation would be the Durbin-Watson test.\nThis tests whether residuals are correlated and produces a summary statistic that ranges between 0 and 4, with 2 signifying no autocorrelation. A value greater than 2 suggesting negative autocorrelation and and value of less than 2 indicating positive autocorrelation.\nIn his excellent text book, Andy Field suggests that you should be concerned with Durbin-Watson test statistics &lt;1 or &gt;3. So let’s see:\n\n#run durbin-watson test\nDW &lt;- durbinWatsonTest(model2)\ntidy(DW)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.41  0.0100           0.227 Durbin-Watson Test two.sided  \n\n\nAs you can see, the DW statistics for our model is 1.41, so some indication of autocorrelation, but perhaps nothing to worry about.\n\n\n4.3.2.5.2 Spatial autocorrelation\nHOWEVER\nWe are using spatially referenced data and as such we should check for spatial-autocorrelation.\nThe first test we should carry out is to map the residuals to see if there are any apparent obvious patterns:\n\n#and for future use, write the residuals out\nmodel2_residuals &lt;- model2 %&gt;%\n  augment(., census_authorities_data_norm)\n\n\nta_model_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model2_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\n\nlibrary(tmap)\n\nWarning: package 'tmap' was built under R version 4.2.3\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\n#now plot the residuals\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(ta_model_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\nVariable(s) \".resid\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nNow, we can immediately see some issues with this…\n\nI have left the data “outside territorial authority” in the data and it has a large negative residual\nChatham Islands has been included and is also a large positive residual.\n\nFor the purposes of this practical let’s remove them and re-run the model…\nNote i have added in median income and it is now significant!\n\ncensus_authorities_data_norm_filter &lt;- census_authorities_data_norm %&gt;%\n  # != means not equal to\n  filter(area_code != \"999\" & area_code != \"067\")\n\nmodel3 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm_filter)\n\nsummary(model3)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm_filter)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6984 -1.5153 -0.4655  1.2802  5.3085 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     2.796e+01\npercent_no_qualification                                        5.152e-01\npercent_home_owner                                             -3.051e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.295e-04\n                                                               Std. Error\n(Intercept)                                                     3.939e+00\npercent_no_qualification                                        7.357e-02\npercent_home_owner                                              5.305e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  8.034e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      7.098 1.45e-09\npercent_no_qualification                                         7.002 2.13e-09\npercent_home_owner                                              -5.752 2.92e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -4.101 0.000122\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 62 degrees of freedom\nMultiple R-squared:  0.7607,    Adjusted R-squared:  0.7491 \nF-statistic: 65.68 on 3 and 62 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that the \\(r^2\\) has jumped to 76%…now Durbin Watson test and map again.\n\nDW &lt;- durbinWatsonTest(model3)\ntidy(DW)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.27 0.00200           0.345 Durbin-Watson Test two.sided  \n\n#and for future use, write the residuals out\nmodel3_residuals &lt;- model3 %&gt;%\n  augment(., census_authorities_data_norm_filter)\n\n\nta_model3_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model3_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\ntm_shape(ta_model3_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\nVariable(s) \".resid\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nThis suggests that there could well be some spatial autocorrelation biasing our model, but can we test for spatial autocorrelation more systematically?\nYes - and some of you will remember this from the practical two weeks ago. We can calculate a number of different statistics to check for spatial autocorrelation - the most common of these being Moran’s I.\n\n#calculate the centroids of all Wards in London\n\nta_model3_data &lt;- ta_model3_data %&gt;%\n  filter(ta2018_v1 != \"999\" & ta2018_v1 != \"067\")\n\ncoordsW &lt;- ta_model3_data%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nWarning in st_centroid.sf(.): st_centroid assumes attributes are constant over\ngeometries of x\n\nplot(coordsW)\n\n\n\n\n\nlibrary(spdep)\n\nLoading required package: sp\n\n\nWarning: package 'sp' was built under R version 4.2.3\n\n\nLoading required package: spData\n\n#Now we need to generate a spatial weights matrix \n#(remember from the lecture a couple of weeks ago). \n#We'll start with a simple binary matrix of queen's case neighbours\n\nLWard_nb &lt;- ta_model3_data %&gt;%\n  poly2nb(., queen=T)\n\n#or nearest neighbours\nknn_wards &lt;-coordsW %&gt;%\n  knearneigh(., k=4)\n\nLWard_knn &lt;- knn_wards %&gt;%\n  knn2nb()\n\n#plot them\nplot(LWard_nb, st_geometry(coordsW), col=\"red\")\n\n\n\n\n\n#create a spatial weights matrix object from these weights\n\nLward.queens_weight &lt;- LWard_nb %&gt;%\n  nb2listw(., style=\"W\")\n\nLward.knn_4_weight &lt;- LWard_knn %&gt;%\n  nb2listw(., style=\"W\")\n\nThe style argument means the style of the output — B is binary encoding listing them as neighbours or not, W row standardised that we saw yesterday.\nNow run a Moran’s I test on the residuals, first using queens neighbours\n\nQueen &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., Lward.queens_weight)%&gt;%\n  tidy()\n\nThen nearest k-nearest neighbours\n\nNearest_neighbour &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., Lward.knn_4_weight)%&gt;%\n  tidy()\n\nQueen\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic   p.value method               alter…¹\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;  \n1     0.342   -0.0154   0.00818      3.95 0.0000387 Moran I test under … greater\n# … with abbreviated variable name ¹​alternative\n\nNearest_neighbour\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic    p.value method              alter…¹\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;  \n1     0.334   -0.0154   0.00622      4.43 0.00000474 Moran I test under… greater\n# … with abbreviated variable name ¹​alternative\n\n\nObserving the Moran’s I statistic for both Queen’s case neighbours and k-nearest neighbours of 4, we can see that the Moran’s I statistic is somewhere between 0.34 and 0.33. Remembering that Moran’s I ranges from between -1 and +1 (0 indicating no spatial autocorrelation) we can conclude that there is some weak to moderate spatial autocorrelation in our residuals.\nThis means that despite passing most of the assumptions of linear regression, we could have a situation here where the presence of some spatial autocorrelation could be leading to biased estimates of our parameters and significance values."
  },
  {
    "objectID": "04_spatial_models.html#spatial-regression-models",
    "href": "04_spatial_models.html#spatial-regression-models",
    "title": "4  Spatial models",
    "section": "4.4 Spatial Regression Models",
    "text": "4.4 Spatial Regression Models\n\n4.4.1 The Spatial Lag (lagged dependent variable) model\nRrunning a Moran’s I test on the residuals from the model suggested that there might be some spatial autocorreation occurring suggesting that places where the model over-predicted the percent of people smoking (those shown in blue in the map above with negative residuals) and under-predicted (those shown in red/orange) occasionally were near to each other.\nWard and Gleditsch (2008) describe this situation (where the value of our \\(y\\) dependent variable - percent people who smoke - may be influenced by neighbouring values) and suggest the way to deal with it is to incorporate a spatially-lagged version of this variable amongst the independent variables on the right-hand side of the equation. In this case, Equation 1 would be updated to look like this:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\rho w_i.y_i + \\epsilon_i\\]\nIn this equation, \\(w\\) is the spatial weights matrix you generated and \\(w_i\\) is vector of all neighbouring areas (in our case, Wards) for any Ward \\(y_i\\).\nIn this model, a positive value for the \\(\\rho w_i.y_i\\) parameter would indicate that the average value for the percent smoking is expected to be higher if, on average, neighbouring authorities also have a higher percentage of people who smoke.\n\n\\(\\rho\\) denotes (represents) the spatial lag\n\nFor more details on running a spatially lagged regression model and interpreting the outputs, see the chapter on spatially lagged models by Ward and Gleditsch (2008) available online here: https://methods.sagepub.com/book/spatial-regression-models/n2.xml\n\n4.4.1.1 Queen’s case lag\nNow run a spatially-lagged regression model with a queen’s case weights matrix\n\nlibrary(spatialreg)\n\nslag_dv_model2_queen &lt;- lagsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(LWard_nb, style=\"C\"), \n               method = \"eigen\")\n\n#what do the outputs show?\ntidy(slag_dv_model2_queen)\n\n# A tibble: 5 × 5\n  term                                         estimate std.e…¹ stati…²  p.value\n  &lt;chr&gt;                                           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 rho                                           2.33e-2 3.11e-2   0.748 4.55e- 1\n2 (Intercept)                                   2.76e+1 3.82e+0   7.23  4.66e-13\n3 percent_no_qualification                      5.08e-1 7.20e-2   7.06  1.67e-12\n4 percent_home_owner                           -3.00e-1 5.16e-2  -5.82  5.99e- 9\n5 census_2018_total_personal_income_median_cu… -3.33e-4 7.77e-5  -4.28  1.85e- 5\n# … with abbreviated variable names ¹​std.error, ²​statistic\n\n#glance() gives model stats but this need something produced from a linear model\n#here we have used lagsarlm()\nglance(slag_dv_model2_queen)\n\n# A tibble: 1 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1     0.763  294.  307.     276.  -141.    66\n\nt&lt;-summary(slag_dv_model2_queen)\n\nsum(t$residuals)\n\n[1] 4.551914e-15\n\n\nRunning the spatially-lagged model with a Queen’s case spatial weights matrix reveals that in this example, there is an insignificant and small effect associated with the spatially lagged dependent variable. However, a different conception of neighbours we might get a different outcome….\nHere:\n\nRho is our spatial lag (0.0232) that measures the variable in the surrounding spatial areas as defined by the spatial weight matrix. We use this as an extra explanatory variable to account for clustering (identified by Moran’s I). If significant it means that the percent of smokers in a unit vary based on the percent of smokers in the neighboring units. If it is positive it means as the percent of smokers increase in the surrounding units so does our central value\nLog likelihood shows how well the data fits the model (like the AIC, which we cover later), the higher the value the better the models fits the data.\nLikelihood ratio (LR) test shows if the addition of the lag is an improvement (from linear regression) and if that’s significant. This code would give the same output…\n\n\nlibrary(lmtest)\nlrtest(slag_dv_model2_queen, model3)\n\n\nLagrange Multiplier (LM) is a test for the absence of spatial autocorrelation in the lag model residuals. If significant then you can reject the Null (no spatial autocorrelation) and accept the alternative (is spatial autocorrelcation)\nWald test (often not used in interpretation of lag models), it tests if the new parameters (the lag) should be included it in the model…if significant then the new variable improves the model fit and needs to be included. This is similar to the LR test and i’ve not seen a situation where one is significant and the other not. Probably why it’s not used!\n\nIn this case we have spatial autocorrelation in the residuals of the model, but the model is not an improvement on OLS — this can also be confirmed with the AIC score (we cover that later) but the lower it is the better. Here it is 293, in OLS (model 3) it was 292. The Log likelihoodfor model 3 (OLS) was -141, here it is -140.\n\n\n\n4.4.2 The Spatial Error Model\nAnother way of coneptualising spatial dependence in regression models is not through values of the dependent variable in some areas affecting those in neighbouring areas (as they do in the spatial lag model), but in treating the spatial autocorrelation in the residuals as something that we need to deal with, perhaps reflecting some spatial autocorrelation amongst unobserved independent variables or some other mis-specification of the model.\nWard and Gleditsch (2008) characterise this model as seeing spatial autocorrelation as a nuisance rather than being particularly informative, however it can still be handled within the model, albeit slightly differently.\nThe spatial error model can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\lambda w_i.\\xi_i + \\epsilon_i\\]\nwhere\n\n\\(\\xi_i\\) is the spatial component of the error terms…in other words the residuals of the values in surrounding spatial units based on the weight matrix.\n\\(\\lambda\\) is a measure of correlation between neighbouring residuals.. “it indicates the extent to which the spatial component of the errors \\(\\xi_i\\) are correlated with one another for nearby observations” as per the weight matrix, Ward and Gleditsch (2008). If there is no correlation between the then this defaults to normal OLS regression\n\nFor more detail on the spatial error model, see Ward and Gleditsch (2008) - https://methods.sagepub.com/Book/spatial-regression-models/n3.xml\nWe can run a spatial error model on the same data below:\n\nsem_1 &lt;- errorsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(LWard_nb, style=\"C\"), \n               method = \"eigen\")\n\n\ntidy(sem_1)\n\n# A tibble: 5 × 5\n  term                                         estimate std.e…¹ stati…²  p.value\n  &lt;chr&gt;                                           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                                   2.53e+1 3.78e+0    6.69 2.28e-11\n2 percent_no_qualification                      5.71e-1 6.86e-2    8.33 0       \n3 percent_home_owner                           -2.94e-1 5.26e-2   -5.58 2.41e- 8\n4 census_2018_total_personal_income_median_cu… -3.04e-4 7.96e-5   -3.82 1.33e- 4\n5 lambda                                        4.32e-1 1.18e-1    3.65 2.62e- 4\n# … with abbreviated variable names ¹​std.error, ²​statistic\n\n\nComparing the results of the spatial error model with the spatially lagged model and the original OLS model, the suggestion here is that the spatially correlated errors in residuals lead to over/under estimations of the coefficients.\nNote, here we can compare to OLS as there is no spatial lag.\nBoth the \\(\\lambda\\) parameter in the spatial error model and the \\(\\rho\\) parameter in the spatially lagged model are larger than their standard errors, so we can conclude that spatial dependence should be borne in mind when interpreting the results of this regression model.\n\n\n4.4.3 Key advice\nThe lag model accounts for situations where the value of the dependent variable in one area might be associated with or influenced by the values of that variable in neighbouring zones (however we choose to define neighbouring in our spatial weights matrix). With our example, smoking in one neighbourhood might be related to smoking in another. You may be able to think of other examples where similar associations may occur. You might run a lag model if you identify spatial autocorrelation in the dependent variable (closer spatial units have similar values) with Moran’s I.\nThe error model deals with spatial autocorrelation (closer spatial units have similar values) of the residuals (vertical distance between your point and line of model – errors – over-predictions or under-predictions) again, potentially revealed though a Moran’s I analysis. The error model is not assuming that neighbouring independent variables are influencing the dependent variable but rather the assumption is of an issue with the specification of the model or the data used (e.g. clustered errors are due to some un-observed clustered variables not included in the model). For example, smoking prevelance may be similar in bordering neighbourhoods because residents in these neighbouring places come from similar socio-economic backgrounds and this was not included as an independent variable in the original model. There is no spatial process (no cross authority interaction) just a cluster of an un-accounted for but influential variable."
  },
  {
    "objectID": "04_spatial_models.html#gwr",
    "href": "04_spatial_models.html#gwr",
    "title": "4  Spatial models",
    "section": "4.5 GWR",
    "text": "4.5 GWR\nRather than spatial autocorrelation causing problems with our model, it might be that a “global” regression model does not capture the full story. In some parts of our study area, the relationships between the dependent and independent variable may not exhibit the same slope coefficient.\nIf this occurs, then we have ‘non-stationarity’ - this is when the global model does not represent the relationships between variables that might vary locally.\n\n4.5.1 Bandwidth\nThis part of the practical will only skirt the edges of GWR, for much more detail you should visit the GWR website which is produced and maintained by Prof Chris Brunsdon and Dr Martin Charlton who originally developed the technique - http://gwr.nuim.ie/\nI should also acknowledge the guide on GWR produced by the University of Bristol, which was a great help when producing this exercise.\n\nlibrary(spgwr)\n\ncoordsW2 &lt;- st_coordinates(coordsW)\n\nta_model3_data_GWR &lt;- cbind(ta_model3_data,coordsW2)\n\nGWRbandwidth &lt;- gwr.sel(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_GWR,  \n                        coords=cbind(ta_model3_data_GWR$X, ta_model3_data_GWR$Y),\n                  adapt=T)\n\nAdaptive q: 0.381966 CV score: 258.7724 \nAdaptive q: 0.618034 CV score: 288.1859 \nAdaptive q: 0.236068 CV score: 223.3048 \nAdaptive q: 0.145898 CV score: 210.9782 \nAdaptive q: 0.03932385 CV score: 212.4756 \nAdaptive q: 0.1017793 CV score: 202.4218 \nAdaptive q: 0.09472045 CV score: 203.8337 \nAdaptive q: 0.111242 CV score: 202.5485 \nAdaptive q: 0.1053937 CV score: 201.7625 \nAdaptive q: 0.1063105 CV score: 201.6931 \nAdaptive q: 0.1074627 CV score: 201.8976 \nAdaptive q: 0.1061616 CV score: 201.6666 \nAdaptive q: 0.1059668 CV score: 201.6645 \nAdaptive q: 0.1057479 CV score: 201.7017 \nAdaptive q: 0.1060531 CV score: 201.6499 \nAdaptive q: 0.1060938 CV score: 201.6545 \nAdaptive q: 0.1060124 CV score: 201.6567 \nAdaptive q: 0.1060531 CV score: 201.6499 \n\nGWRbandwidth\n\n[1] 0.1060531\n\n\nSetting adapt=T here means to automatically find the proportion of observations for the weighting using k nearest neighbours (an adaptive bandwidth), False would mean a global bandwidth and that would be in meters (as our data is projected).\nOccasionally data can come with longitude and latitude as columns (e.g. WGS84) and we can use this straight in the function to save making centroids, calculating the coordinates and then joining - the argument for this is longlat=TRUE and then the columns selected in the coords argument e.g. coords=cbind(long, lat). The distance result will then be in KM.\nThe optimal bandwidth is about 0.106 meaning 10.6% of all the total spatial units should be used for the local regression based on k-nearest neighbours. Which is about 7 of the 66 territorial authorities.\nThis approach uses cross validation to search for the optimal bandwidth, it compares different bandwidths to minimise model residuals — this is why we specify the regression model within gwr.sel(). It does this with a Gaussian weighting scheme (which is the default) - meaning that near points have greater influence in the regression and the influence decreases with distance - there are variations of this, but Gaussian is a fine to use in most applications. To change this we would set the argument gweight = gwr.Gauss in the gwr.sel() function — gwr.bisquare() is the other option. We don’t go into cross validation in this module.\nHowever we could set either the number of neighbours considered or the distance within which to considered points ourselves, manually, in the gwr() function below.\nTo set the number of other neighbours considered simply change the adapt argument to the value you want - it must be the number of neighbours divided by the total (e.g. to consider 20 neighbours it would be 20/66 and you’d use the value of 0.30)\nThe set the bandwidth, remove the adapt argument and replace it with bandwidth and set it, in this case, in meters.\nTo conclude, we can:\n\nset the bandwidth in gwr.sel() automatically using:\n\nthe number of neighbors\na distance threshold\n\nOr, we can set it manually in gwr() using:\n\na number of neighbors\na distance threshold\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBUT a problem with setting a fixed bandwidth is we assume that all variables have the same relationship across the same space (using the same number of neighbours or distance)…(such as rate_of_job_seekers_allowance_jsa_claimants_2015 and percent_with_level_4_qualifications_and_above_2011). We can let these bandwidths vary as some relationships will operate on different spatial scales…this is called Multiscale GWR and Lex Comber recently said that all GWR should be Multisacle (oops!). We have already covered a lot here so i won’t go into it. If you are interested Lex has a good tutorial on Multiscale GWR\n\n\n\n\n4.5.2 Model\n\n#run the gwr model\ngwr.model = gwr(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_GWR,  \n                        coords=cbind(ta_model3_data_GWR$X, ta_model3_data_GWR$Y), \n                adapt=GWRbandwidth,\n                #matrix output\n                hatmatrix=TRUE,\n                #standard error\n                se.fit=TRUE)\n\n#print the results of the model\ngwr.model\n\nCall:\ngwr(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = ta_model3_data_GWR, coords = cbind(ta_model3_data_GWR$X, \n        ta_model3_data_GWR$Y), adapt = GWRbandwidth, hatmatrix = TRUE, \n    se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.1060531 (about 6 of 66 data points)\nSummary of GWR coefficient estimates at data points:\n                                                                      Min.\nX.Intercept.                                                    3.3703e+00\npercent_no_qualification                                        4.1815e-01\npercent_home_owner                                             -4.4357e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.2994e-04\n                                                                   1st Qu.\nX.Intercept.                                                    1.2399e+01\npercent_no_qualification                                        4.8399e-01\npercent_home_owner                                             -3.1838e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -5.2518e-04\n                                                                    Median\nX.Intercept.                                                    1.7101e+01\npercent_no_qualification                                        5.2129e-01\npercent_home_owner                                             -2.4756e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.6083e-05\n                                                                   3rd Qu.\nX.Intercept.                                                    3.4618e+01\npercent_no_qualification                                        5.8085e-01\npercent_home_owner                                             -1.8668e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -2.1233e-05\n                                                                      Max.\nX.Intercept.                                                    4.8631e+01\npercent_no_qualification                                        6.9995e-01\npercent_home_owner                                             -1.6115e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over  1.9089e-04\n                                                                Global\nX.Intercept.                                                   27.9622\npercent_no_qualification                                        0.5152\npercent_home_owner                                             -0.3051\ncensus_2018_total_personal_income_median_curp_15years_and_over -0.0003\nNumber of data points: 66 \nEffective number of parameters (residual: 2traceS - traceS'S): 25.09782 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 40.90218 \nSigma (residual: 2traceS - traceS'S): 1.503921 \nEffective number of parameters (model: traceS): 19.93781 \nEffective degrees of freedom (model: traceS): 46.06219 \nSigma (model: traceS): 1.417184 \nSigma (ML): 1.183931 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 272.3116 \nAIC (GWR p. 96, eq. 4.22): 229.5246 \nResidual sum of squares: 92.51173 \nQuasi-global R2: 0.9204215 \n\n\nThe output from the GWR model reveals how the coefficients vary across the 66 territorial authorities New Zealand. You will see how the global coefficients are exactly the same as the coefficients in the earlier lm model. In this particular model, if we take percent with no qualification, we can see that the coefficients range from a minimum value of 0.41 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.41) to 0.69 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.69).\nYou will notice that the R-Squared value (Quasi global R-squared) has improved - this is not uncommon for GWR models, but it doesn’t necessarily mean they are definitely better than global models. The small number of cases (spatial areas) under the kernel means that GW models have been criticised for lacking statistical robustness.\nThe best way to compare models is with the AIC (Akaike Information Criterion) or for smaller sample sizes the sample-size adjusted AICc, especially when you number of points is less than 40! Which it will be in GWR. The models must also be using the same data and be over the same study area!\nAIC is calculated using the:\n\nnumber of independent variables\nmaximum likelihood estimate of the model (how well the model reproduces the data).\n\nThe lower the value the better the better the model fit is, see scribbrif you want to know more here..although this is enough to get you through most situations.\nCoefficient ranges can also be seen for the other variables and they suggest some interesting spatial patterning.\n\n\n4.5.3 Map coefficients\nTo explore this we can plot the GWR coefficients for different variables. Firstly we can attach the coefficients to our original dataframe - this can be achieved simply as the coefficients for each territoiral authority appear in the same order in our spatial points dataframe as they do in the original dataframe.\n\nresults &lt;- as.data.frame(gwr.model$SDF)\nnames(results)\n\n [1] \"sum.w\"                                                                \n [2] \"X.Intercept.\"                                                         \n [3] \"percent_no_qualification\"                                             \n [4] \"percent_home_owner\"                                                   \n [5] \"census_2018_total_personal_income_median_curp_15years_and_over\"       \n [6] \"X.Intercept._se\"                                                      \n [7] \"percent_no_qualification_se\"                                          \n [8] \"percent_home_owner_se\"                                                \n [9] \"census_2018_total_personal_income_median_curp_15years_and_over_se\"    \n[10] \"gwr.e\"                                                                \n[11] \"pred\"                                                                 \n[12] \"pred.se\"                                                              \n[13] \"localR2\"                                                              \n[14] \"X.Intercept._se_EDF\"                                                  \n[15] \"percent_no_qualification_se_EDF\"                                      \n[16] \"percent_home_owner_se_EDF\"                                            \n[17] \"census_2018_total_personal_income_median_curp_15years_and_over_se_EDF\"\n[18] \"pred.se.1\"                                                            \n[19] \"coord.x\"                                                              \n[20] \"coord.y\"                                                              \n\n\n\n#attach coefficients to original\n\n\nta_model3_data_GWR_results &lt;- ta_model3_data_GWR %&gt;%\n  mutate(coefqualification = results$percent_no_qualification,\n         coefhomeowner = results$percent_home_owner,\n         coefincome = results$census_2018_total_personal_income_median_curp_15years_and_over_se)\n\n\ntm_shape(ta_model3_data_GWR_results) +\n  tm_polygons(col = \"coefqualification\", \n              palette = \"RdBu\", \n              alpha = 0.5)\n\n\n\n\nTaking the first coefficient we see that this varies across the country. A possible trend to explore further might be the inclusion or distance to tobacco retailers - both Wellington and Auckland have some of the highest coefficients, meaning a 1 unit (percent) change in no qualification returns a higher rise in percent of people smoking.\n\n\n4.5.4 Significance\nOf course, these results may not be statistically significant across the whole of New Zealand. Roughly speaking, if a coefficient estimate is more than 2 standard errors away from zero, then it is “statistically significant”.\nRemember from earlier the standard error is “the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for 1% increase in percent with no qualifications, while the model says we might expect GSCE scores to drop by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.”\nTo calculate standard errors, for each variable you can use a formula similar to this:\n\n#run the significance test\n# abs gets the absolute vale - negative values are converted into positive value\nsigTest = abs(gwr.model$SDF$percent_no_qualification)- 2* gwr.model$SDF$percent_no_qualification_se\n\n\n#store significance results\nta_model3_data_GWR_results &lt;- ta_model3_data_GWR_results %&gt;%\n  mutate(GWRpercent_no_qualification_sig = sigTest)\n\n\ntm_shape(ta_model3_data_GWR_results) +\n  tm_polygons(col = \"GWRpercent_no_qualification_sig\", \n              palette = \"RdYlBu\")\n\n\n\n\nIf this is greater than zero (i.e. the estimate is more than two standard errors away from zero), it is very unlikely that the true value is zero, i.e. it is statistically significant (at nearly the 95% confidence level)\nThis is a combination of two ideas:\n\n95% of data in a normal distribution is within two standard deviations of the mean\nStatistical significance in a regression is normally measured at the 95% level. If the p-value is less than 5% — 0.05 — then there’s a 95% probability that a coefficient as large as you are observing didn’t occur by chance\n\nCombining these two means if…\n\nthe coefficient is large in relation to its standard error and\nthe p-value tells you if that largeness is statistically acceptable - at the 95% level (less than 5% — 0.05)\n\nYou can be confident that in your sample, nearly all of the time, that is a real and reliable coefficient value."
  },
  {
    "objectID": "04_spatial_models.html#including-spatial-features",
    "href": "04_spatial_models.html#including-spatial-features",
    "title": "4  Spatial models",
    "section": "4.6 Including spatial features",
    "text": "4.6 Including spatial features\nI mentioned at the start we could also try and include some spatial features within our model - for example the density of shops that sell tobacco? In a sense this is similar to what is termed a spatially omitted covariate… typically this means that the nearer something is the more influence it might have on our model.\nHowever, we could only really calculate distance to tobacco shops from the centroid of the territorial authority - which is a possible variable. But instead we could use density of shops per territorial authority!\nHere, i will use open street map to calculate a density. I have downloaded the data from Geofabrik\n\nOSM_pois &lt;- st_read(\"prac4_data/new-zealand-latest-free.shp/gis_osm_pois_a_free_1.shp\")\n\nReading layer `gis_osm_pois_a_free_1' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_24\\Spatial analysis of public health data\\prac4_data\\new-zealand-latest-free.shp\\gis_osm_pois_a_free_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 59185 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -176.6404 ymin: -52.59086 xmax: 178.5443 ymax: -34.42668\nGeodetic CRS:  WGS 84\n\nOSM_pois_filter &lt;- OSM_pois %&gt;%\n  filter(fclass == \"supermarket\" | fclass ==\"convenience\")%&gt;%\n  st_transform(., 2135)\n\nNow let’s intersect with the territorial authority boundaries and calculate a density.\nNote that the districts object is in the correct CRS, however i beleive this is due to naming differences so we will transform it just incase.\n\nta &lt;- ta %&gt;%\n   st_transform(., 2135)\n\nstores_joined &lt;- ta %&gt;%\n  mutate(n = lengths(st_intersects(., OSM_pois_filter)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow, we could use this in our model!\nOn the 1st July 2024 New Zealand intends to reduce the number of tobacco retailers from 6,000 to just 599! With a maximum number per area according to the table set on Ministry of Health website."
  },
  {
    "objectID": "05_datasets.html",
    "href": "05_datasets.html",
    "title": "5  Datasets",
    "section": "",
    "text": "Health data can be hard to find online. Here are some interesting datasets i have come across:\n\nUK Small area mental health index / other mental health data\nUK Understanding society health and wellbeing\nUK Police data (e.g. for looking at violence and mental wellbeing)"
  }
]